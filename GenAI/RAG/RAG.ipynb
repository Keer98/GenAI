{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1717689c-6b8e-49f8-ab53-437779045e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: chromadb in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.5.23)\n",
      "Requirement already satisfied: langchain in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.115.5)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (3.7.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.28.2)\n",
      "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (1.68.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (4.2.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.13.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (3.11.6)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.26 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.1.143)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-groq) (0.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from build>=1.0.3->chromadb) (24.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\keert\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 sentence_transformers chromadb langchain langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c54fff7c-8450-4ab0-b79e-6cdb9910f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        api_key=\"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\",\n",
    "        temperature=0,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "        max_retries=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bd14685-8e98-4af0-8812-2f8c8645c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "from docx import Document\n",
    "\n",
    "PDF_FOLDER = 'Documents'\n",
    "\n",
    "def extract_text_from_pdf_with_pages(pdf_path, pdf_file):\n",
    "    global text_by_page\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_number, page in enumerate(reader.pages):\n",
    "                text_by_page.append((page_number + 1, page.extract_text(),pdf_file))  # Page numbers are 1-indexed\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text_by_page\n",
    "\n",
    "def extract_text_from_docx(docx_path, docx_file):\n",
    "    global text_by_page\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        full_text = []\n",
    "        for paragraph in doc.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        text_by_page.append((1, \"\\n\".join(full_text), docx_file))  # Treat as a single \"page\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {docx_path}: {e}\")\n",
    "    return text_by_page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c092de12-b611-461a-98ee-1cb292852c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_processing(file_name):\n",
    "    if file_name.endswith(\".pdf\"):  # Process only PDF files\n",
    "        pdf_path = os.path.join(PDF_FOLDER, file_name)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "\n",
    "        # Extract text\n",
    "        #text = extract_text_from_pdf(pdf_path)\n",
    "        text_by_page = extract_text_from_pdf_with_pages(pdf_path,file_name)\n",
    "            \n",
    "        if not text_by_page:  # Skip empty or unreadable files\n",
    "            print(f\"Skipping {pdf_file}: No text extracted.\")\n",
    "            return\n",
    "    elif file_name.endswith(\".docx\"):  # Process DOCX files\n",
    "        docx_path = os.path.join(PDF_FOLDER, file_name)\n",
    "        print(f\"Processing DOCX {file_name}...\")\n",
    "\n",
    "        # Extract text from DOCX\n",
    "        text_by_page = extract_text_from_docx(docx_path, file_name)\n",
    "\n",
    "        if not text_by_page:  # Skip empty or unreadable files\n",
    "            print(f\"Skipping {file_name}: No text extracted.\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95576e62-aa1a-4dbf-b9b4-4c93038473b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DOCX dynamic_data.docx...\n",
      "Processing Finetunung_LLM.pdf...\n",
      "Processing genai-principles.pdf...\n"
     ]
    }
   ],
   "source": [
    "text_by_page = []\n",
    "# Iterate over PDFs in the folder\n",
    "for file_name in os.listdir(PDF_FOLDER):\n",
    "    file_processing(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22fae267-37a8-418f-a8cc-5cdd4eb9cdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, \"Resolving challenges in a Retrieval-Augmented Generation (RAG) pipeline at the enterprise level involves a combination of advanced technical implementations, infrastructure optimization, and organizational practices. Here’s how each challenge can be addressed:\\n\\n1. Retriever Efficiency\\nChallenge: Retrieving relevant documents from large corpora can be computationally expensive and time-intensive.\\n\\nSolutions:\\nEfficient Indexing:\\nUse high-performance vector search engines like FAISS, Pinecone, or Weaviate to create optimized indexes.\\nEmploy hierarchical indexing or partitioning to speed up retrieval.\\nSparse + Dense Fusion:\\nCombine traditional methods like BM25 with dense vector retrieval for a hybrid approach that balances efficiency and accuracy.\\nPre-filtering:\\nApply metadata-based filtering (e.g., categories, tags) to reduce the search space before dense similarity search.\\nAsynchronous Updates:\\nIncrementally update the knowledge base with new documents using asynchronous indexing to minimize downtime.\\nBatch Query Processing:\\nFor high-throughput systems, batch queries to reduce redundant computations and improve resource utilization.\\n2. Knowledge Grounding\\nChallenge: Ensuring the generated response is grounded in the retrieved context and not hallucinated.\\n\\nSolutions:\\nFaithful Generation Techniques:\\nFine-tune the generative model on domain-specific datasets to improve its ability to stick to provided contexts.\\nUse RAG-Token models to ensure the generator dynamically attends to tokens from all retrieved documents.\\nContent Verification:\\nImplement post-generation verification by comparing the generated output against the retrieved documents using natural language inference (NLI) models.\\nRetriever-Generator Interaction:\\nUse iterative refinement where the generator highlights uncertainties, and the retriever fetches more specific documents in response.\\nDomain-Specific Constraints:\\nAdd constraints during generation, such as limiting outputs to specific vocabularies or document phrases.\\n3. Document Relevance\\nChallenge: The retriever might fetch irrelevant or partially relevant documents.\\n\\nSolutions:\\nRetriever Fine-tuning:\\nFine-tune the retriever on enterprise-specific datasets to align retrieval with the organization's domain and context.\\nRe-ranking with Advanced Models:\\nUse transformer-based ranking models (e.g., BERT-based cross-encoders) to re-rank retrieved documents based on query relevance.\\nFeedback Loops:\\nCollect user feedback to refine retriever behavior iteratively. For instance, capture user corrections or preferences and incorporate them into retraining.\\nContextual Filtering:\\nFilter results based on query intent using additional layers of intent classification and document tagging.\\n4. Scalability\\nChallenge: Scaling RAG for very large corpora and handling high traffic.\\n\\nSolutions:\\nDistributed Infrastructure:\\nUse distributed systems like Apache Spark, Elasticsearch, or cloud-based solutions like AWS OpenSearch to handle large-scale indexing and retrieval.\\nShard and Replicate:\\nShard the knowledge base into smaller, manageable segments and replicate for high availability.\\nDynamic Scaling:\\nImplement auto-scaling for retrieval and generation pipelines to handle traffic spikes without compromising performance.\\nMemory Optimization:\\nUse memory-efficient models and techniques like quantization for retriever and generator embeddings to reduce computational overhead.\\n5. Latency\\nChallenge: Combining retrieval and generation can introduce delays, especially in real-time applications.\\n\\nSolutions:\\nCaching:\\nCache frequently accessed query results to reduce retrieval time.\\nCache common query-document pairs and precompute embeddings for high-frequency queries.\\nParallel Processing:\\nExecute retrieval and pre-processing steps in parallel to reduce total pipeline time.\\nModel Optimization:\\nUse lightweight models like DistilBERT or ALBERT for retrieval and generation.\\nApply techniques like pruning and knowledge distillation to reduce model size while maintaining accuracy.\\nLatency-Budgeted Design:\\nFor real-time applications, limit the number of documents retrieved (e.g., top-k) and keep context inputs to manageable sizes.\\n6. Knowledge Updates\\nChallenge: Incorporating newly available information into the pipeline in near real-time.\\n\\nSolutions:\\nIncremental Indexing:\\nRegularly add new documents to the index without rebuilding it entirely using incremental indexing features of vector databases.\\nEvent-Driven Updates:\\nAutomate knowledge updates using event-driven pipelines (e.g., Apache Kafka or AWS Lambda) to process new data streams in real-time.\\nDomain-Specific Knowledge Bases:\\nMaintain separate, frequently updated knowledge bases for critical domains and integrate them dynamically into the pipeline.\\n7. Cost Management\\nChallenge: RAG pipelines can be expensive due to compute and storage requirements.\\n\\nSolutions:\\nCloud-Based Solutions:\\nUse pay-as-you-go cloud services like AWS, Google Cloud, or Azure for hosting infrastructure.\\nModel Compression:\\nReduce compute costs by employing techniques like model quantization, pruning, or distillation.\\nCost-Efficient Retrieval:\\nUse sparse retrievers like BM25 for queries where exact matches are likely, reserving dense retrieval for complex queries.\\n8. Security and Privacy\\nChallenge: Ensuring sensitive enterprise data is secure in retrieval and generation processes.\\n\\nSolutions:\\nAccess Control:\\nImplement role-based access control (RBAC) to restrict access to sensitive data.\\nData Encryption:\\nEncrypt data at rest and in transit to ensure security.\\nPrivate Deployment:\\nDeploy retrievers and generators on on-premises infrastructure or within secure VPCs for sensitive applications.\\nAuditing:\\nLog retrieval and generation activity for audit and compliance purposes.\\nEnterprise-Ready Implementation\\nSteps to Make RAG Enterprise-Ready:\\n\\nData Preparation:\\nCurate and preprocess enterprise knowledge bases for retrieval.\\nDomain Adaptation:\\nFine-tune both retriever and generator on domain-specific datasets.\\nInfrastructure:\\nLeverage scalable, secure, and cost-effective cloud or hybrid infrastructure.\\nMonitoring:\\nMonitor pipeline performance with observability tools like Prometheus or Grafana.\\nContinuous Improvement:\\nRegularly refine models, pipelines, and processes using user feedback and new datasets.\\nBy systematically addressing each challenge, RAG pipelines can deliver high-quality, reliable, and efficient results at the enterprise level.\", 'dynamic_data.docx'), (1, 'Efficient Fine-tuning Large Language Models for\\nKnowledge-Aware Response Planning\\nMinh Nguyen1⋆, Kishan K C2, Toan Nguyen2, Ankit Chadha2, and Thuy Vu2\\x00\\n1Department of Computer Science, University of Oregon, OR, USA\\n2Amazon Alexa AI, CA, USA\\nminhnv@cs.uoregon.edu\\n{ckshan,amztoan,ankitrc,thuyvu }@amazon.com\\nAbstract. Large Language Models (LLMs) have shown impressive emergent\\nlanguage capabilities, especially in applications with high ambiguity, such as lan-\\nguage reasoning and knowledge consolidation. However, previous work explores\\nthe use of LLMs for acquiring information using either parametric or external\\nknowledge, which might lead to serious issues such as hallucination. Toward\\nsolving these issues, we present a novel approach of knowledge-aware response\\nplanning (KARP) and propose a novel framework that employs (i) a knowledge re-\\ntriever to obtain relevant information from web documents or databases for a given\\nuser query, and (ii) a robust fine-tuning strategy for LLMs to exploit the retrieved\\nexternal knowledge for planning a final response. Experimental results show that\\nour proposed framework can provide natural, concise answers for open-domain\\nquestions with high accuracy.\\nKeywords: Knowledge-Aware Response Planning ·Question Answering ·Large\\nLanguage Models · Fine-tuning.\\n1 Introduction\\nGeneral question answering (QA), a crucial natural language processing (NLP) task, is\\noften regarded as AI-complete [62, 8]; that is, QA will only be considered solved once\\nall the challenging problems in artificial intelligence (AI) have been addressed. Several\\nvirtual response assistants, including Google Assistant, Amazon Alexa, and Apple’s\\nSiri, have integrated state-of-the-art QA technologies, allowing them to understand and\\ngenerate responses in natural languages, providing valuable services to users. However,\\ngeneral QA still presents significant challenges, primarily due to the inherent difficulties\\nin reasoning with natural language, including aspects like commonsense and general\\nknowledge. Past research has explored the use of Large Language Models (LLMs) for\\ngeneral QA, predominantly leveraging either parametric (e.g., ChatGPT3) or external\\n(e.g., WebGPT[38]) knowledge sources. This method, however, can lead to considerable\\ncomplications, including hallucination - the generation of plausible but incorrect or\\nunverified information. To address these challenges, this paper introduces the concept\\n⋆This work was completed while the author was an intern at Amazon Alexa AI.\\n3https://chat.openai.com/chat', 'Finetunung_LLM.pdf'), (2, '2 Nguyen et al.\\nq: What college offers chiropractic ?\\nc1:New York Chiropractic College offers 1 Chiropractic Degree program. It’s a private\\nuniversity in a far away town. In 2015, 173 students graduated in the study area of\\nChiropractic with students earning 173 Doctoral degrees.\\na1: New York Chiropractic college offers chiropractic.\\nc2:Chiropractic care is also essential for college students who want to stay healthy. The central\\nnervous system is based in the spinal column, so correcting subluxations (misalignments)\\nof the spine is important, no matter how old you are. Holt Chiropractic in Port Orchard,\\nWA provides expert chiropractic care to students of all ages.\\na2: Holt Chiropractic College offers chiropractic.\\nc3:Howell Township is a township in Monmouth County, New Jersey, United States. As\\nof the 2010 United States Census, the township’s population was 51,075, reflecting an\\nincrease of 2,172 from the 48,903 counted in the 2000 Census.\\na3: Howell Township College offers chiropractic.\\nTable 1: Generated answers for a question qwith different context passages c1(relevant),\\nc2(quasi-relevant), and c3(irrelevant) from MS MARCO QA NLG test set [39]. Answers\\na1,a2, and a3are generated by GenQA [17].\\nof Knowledge-Aware Response Planning (KARP) for general QA along with a novel\\nframework that combines a knowledge retriever with a robust fine-tuning strategy for\\nLLMs. In particular, the problem of KARP can be defined as follows. Given a user\\nquery and a prompt containing external knowledge, the goal is to develop a model that\\ncan consolidate a response that must be crafted not just from the externally sourced\\ninformation, but also from the model’s inherent parametric knowledge. This is different\\nfrom the previous work that aim to generate a response by either harnessing parametric\\nknowledge (e.g., ChatGPT) or retrieving from external knowledge such as knowledge\\nbases [2, 3, 65, 51], web documents [68, 6, 67, 7, 13], or a provided context [15, 45, 59,\\n10].\\nWith the emergent abilities of LLMs [61], generative QA systems, in which answers\\nare produced by a generative LLM, have been explored to improve the performance\\nof QA [21, 49, 43, 19, 27, 18, 12, 37]. In paritcular, previous work typically employs\\npre-trained LLMs with encoder-decoder architectures such as BART [28] and T5 [44],\\nwhere the encoder consumes a given question and a required relevant context as input for\\nthe decoder to generate an answer to the question [24, 17]. On one hand, the similarity\\nbetween generative QA and the pre-training tasks of LLMs enables transfer learning\\nto improve QA performance. On the other hand, the generative formulation allows for\\nflexibility in handling various types of QA problems (e.g., extractive QA, multiple-choice\\nQA) [24]. However, a well-known issue that has been shown to occur with the generative\\nmodels is hallucination [35, 50, 53], where the models generate statements that are\\nplausible looking but factually incorrect. Additionally, if the answers are composed by\\na pretrained LLM without external knowledge, i.e., using parametric knowledge, the\\ninformation contained in the answers might be outdated and no longer valid. For example,', 'Finetunung_LLM.pdf'), (3, 'Efficient Fine-tuning Large Language Models 3\\nthe answer for the question “Which country is the reigning World Cup champion?” will\\nchange through time.\\nRecent works such as GenQA [17] and WebGPT [38] mitigate these issues by\\nemploying an information retrieval component, which is responsible for collecting web\\ncontent to compose an answer for a given question. Formally, given a question qand\\na retrieved web content c, the model is trained to take (q, c)as input to produce a\\nresponse a=fθ(q, c), where fθdenotes the corresponding LLM with the parameters θ.\\nUnfortunately, fθmay merely learn to copy/synthesize information from cto produce\\naifcoften contains necessary information for correctly answering the question qin\\ntraining data such as MS MARCO QA NLG [39]4. As a result, the model may fail\\nto provide a correct answer for a given question if the retrieved content is missing\\nor contains (quasi-)irrelevant information (see Table 1). In other words, performance\\nof these retrieval-based QA models are limited to an upper bound by the knowledge\\nretriever.\\nIn this work, we address such issues in building a generative QA model. First, we\\nutilize a knowledge retriever that employs Optimal Transport to selectively identify\\nrelevant content from web documents or databases for a given user query. Second,\\nwe propose a novel fine-tuning strategy specially designed for LLMs, which combines\\nexternal knowledge, i.e., provided by the knowledge retriever and the intrinsic pre-trained\\nknowledge in LLMs, wherever possible, to generate informed responses.\\nParticularly, we propose the knowledge retriever as a dense passage retriever (DPR)\\nmodel. Our proposed DPR model performs an alignment between a given question and\\na text passage via Optimal Transport to find relevant information in the passage for\\ndetermining its correctness. The relevant context in the passage will then be used to\\nproduce a correctness score for ranking. In this way, we can obtain top ktext passages\\nfrom databases/web documents, which are treated as external knowledge in our frame-\\nwork. Different from GenQA and WebGPT that follows a single-style “ a=fθ(q, c)”\\nfinetuning strategy, we propose to employ a multi-style finetuning strategy, where both\\n“a=fθ(q, c)” and “ a=fθ(q)” are used to train the model. The latter intentionally\\nexcludes the external knowledge cfrom the input to encourage the model to retrieve its\\nown knowledge from the model parameters θ, which have been pretrained on massive\\nunlabeled text data [28, 44, 11, 56]. To combine the two finetuning styles, we propose\\nto finetune the LLM with “ a=fθ(q, c)”, and sequentially finetune the model with\\n“a=fθ(q)”. At test time, we use the “ a=fθ(q, c)” style to make predictions. Ex-\\nperimental results show that our proposed finetuning strategy significantly improves\\nthe performance compared to the baselines on MS MARCO QA NLG, demonstrat-\\nting the effectiveness of our proposed method. Finally, we scale up our framework to\\nfurther improve the QA performance by training the model i) with “ a=fθ(q, c)” on\\nQA datasets such as SQUAD [45] ( cis a context passage), MCTest [48] ( cconsists of\\nmultiple choices), Anthropic [1] ( cis the previous question in a conversation), and ii)\\nwith “ a=fθ(q)” on QA datasets such as WikiQA [69] and Wdrass [73].\\n4All answers ain MS MARCO QA NLG are written by human annotators based on summarizing\\nanswer information in context passages c.', 'Finetunung_LLM.pdf'), (4, '4 Nguyen et al.\\nOur experiments show that the resulting system behaves as a knowledge aware\\nresponse planner that provides natural, concise answers for open-domain questions with\\nhigh accuracy.\\nFig. 1: Overview of our proposed framework for KARP. The blue and orange arrows\\nrepresent the finetuning and inference processes of our model respectively.\\n2 Proposed Method\\n2.1 Knowledge-Aware Response Planning\\nThe problem of Knowledge-Aware Response Planning (KARP) can be outlined as\\nfollows: Given a user query and a prompt loaded with external knowledge, the aim is to\\nbuild a model capable of formulating a response. This response should be planned not\\nonly from the external information provided but also derived from the model’s inherent\\nparametric knowledge.\\nTo this end, our proposed framework for KARP consists of (i) a knowledge retriever\\nand (ii) a generative LLM-based question answering model. An overview of our frame-\\nwork is shown in Figure 1. Details regarding the knowledge retriever and the generative\\nQA model are presented in section 2.2 and 2.3, respectively.\\n2.2 Knowledge Retriever\\nOur knowledge retriever functions as a dense passage retrieval (DPR) system. Given\\na question qand a group of Ntext passages C={c1, c2, . . . , c N}, the goal of DPR\\nis to determine the correct answer passages A⊂Cby learning a reranking function\\nr:Q×ϕ(C)→ϕ(C), where Qrepresents the set of questions and ϕ(C)represents all\\nthe possible orderings of C. The intent is to place the answer passages Aat the top of\\nthe ranking produced by the function r. The reranker ris typically a pointwise network\\nf(q, ci), such as TANDA [13], which learns to assign a correctness score pi∈(0,1)to\\neach text passage cifor ranking purposes. Our focus lies on the contextual DPR, where', 'Finetunung_LLM.pdf'), (5, 'Efficient Fine-tuning Large Language Models 5\\nsupplementary context, like surrounding context, is used to more accurately ascertain\\nthe validity score of an answer passage.\\nOur knowledge retriever consists of three primary elements: i) Encoding, ii) Question-\\nContext Alignment with Optimal Transport (OT), and iii) Answer-Context Dependencies.\\nThe diagram of our suggested model can be seen in Figure 2.\\nEncoding We are provided with a question represented as q= [wq\\n1, wq\\n2, . . . , wq\\nTq]\\nwith Tqwords and a set of Ntext passages C={c1, c2, . . . , c N}retrieved from\\na search engine. Each passage, denoted as ci= [wc\\n1, wc\\n2, . . . , wc\\nTc], consists of Tc\\nwords. In this work, we consider previous and next passages cprev,cnext as additional\\ncontext for each candidate passage c∈C. To create the input for our DPR model,\\nwe concatenate the question, answer passage, and context passages into a single input\\nsequence: [q;c;cprev;cnext]. This combined sequence is then passed through a pre-\\ntrained language model (PLM), e.g., RoBERTa [33], to obtain contextualized word\\nembeddings. Additionally, we employ distinct segment embeddings for the question,\\nanswer passage, and context passages. These segment embeddings, which are randomly\\ninitialized and trainable during training, are added to the initial word embeddings in\\nthe first layer of the PLM. For simplicity, let [wq\\n1,wq\\n2, . . . , wq\\nTq]and[wc\\n1,wc\\n2, . . . , wc\\nTc]\\nrepresent the sequences of word representations obtained from the last layer of the PLM\\nfor the question qand the answer passage c∈C, respectively.\\nPretrained Language Model[CLS] question [SEP]   answer passage  [SEP] prev_context [SEP] next_contextInter-ContextDependencies\\nAlignmentvia Optimal TransportQuestionAnswer/Context\\n. . .\\nRelevant Context\\nGraph Convolutional Network\\nAS2 prediction\\nFig. 2: A diagram depicting the knowledge retriever in our framework for KARP.', 'Finetunung_LLM.pdf'), (6, '6 Nguyen et al.\\nQuestion-Context Alignment with OT In this section, we present our approach for\\nidentifying relevant context within the answer passage and its surrounding passages\\nbased on the alignment of words with the question. Specifically, we introduce the use\\nof Optimal Transport (OT) [36, 9] to address the task of aligning the question with the\\ncontext for DPR.\\nOT is a well-established technique used to transfer probability from one distribution\\nto another by establishing an alignment between two sets of points. In the discrete\\nsetting, we are provided with two probability distributions, denoted as pXandpY,\\ndefined over two sets of points, namely X={xi}n\\ni=1andY={yj}m\\nj=1(P\\nipxi= 1\\nandP\\njpyj= 1). Additionally, a distance function D(x, y) :X×Y→R+is given\\nto quantify the dissimilarity between any two points xandy. The objective of OT is to\\ndetermine a mapping that transfers the probability mass from the points in {xi}n\\ni=1to the\\npoints in {yj}m\\nj=1, while minimizing the overall cost associated with this transportation.\\nFormally, this involves finding the transportation matrix πXY∈R+n×mthat minimizes\\nthe following transportation cost:\\ndXY=X\\n1≤i≤n\\n1≤j≤mD(xi, yj)πXYij, (1)\\nso that πXY1m=pXandπT\\nXY1n=pY. The transportation matrix πXYsignifies the\\nbest matching between the sets of points XandY, where each row iin the matrix\\nindicates the optimal alignment from a point xi∈Xto each point yj∈Y.\\nIn our problem of aligning the question with the answer passage, we treat the question\\nqand the answer/context passage cas two point sets: {wq\\ni}Tq\\ni=1and{wc\\ni}Tc\\ni=1respectively\\n(each word is a point)5. To determine the probability distributions for these word sets, we\\npropose calculating the word frequencies and then normalizing the sum of frequencies.\\nSpecifically, the probability distribution for the question is obtained by:\\npwq\\ni=freq(wq\\ni)\\nPTq\\ni′=1freq(wq\\ni′)(2)\\nThe frequency freq(wq\\ni)corresponds to the number of occurrences of the word wq\\ni\\nin the training data’s questions. The same approach is applied to the answer/context\\npassage. To handle unseen words during testing, we utilize Laplace smoothing to assign\\na non-zero probability. Moving on, we estimate the distance between two words wq\\ni∈q\\nandwc\\nj∈cby measuring their semantic divergence, which involves computing the\\nEuclidean distance between their contextualized representations obtained from the PLM:\\nD(wq\\ni, wc\\nj) =||wq\\ni−wc\\nj||. The Sinkhorn-Knopp algorithm is then efficiently employed\\nto solve for the optimal transportation matrix πXY(in this case, πqcfor the question q\\nand the passage c) [55, 9]. Finally, we obtain the relevant context rcfor the passage cby\\ntaking the union of words wc\\njthat have the highest transportation probabilities:\\nrc=Tq[\\ni=1{wc\\nj|j=argmax1≤j′≤Tcπqcij′} (3)\\n5Before performing the alignment, we remove stopwords and punctuation marks from both sets\\nof words.', 'Finetunung_LLM.pdf'), (7, 'Efficient Fine-tuning Large Language Models 7\\nTo compute the representation for the passage c, we take the average sum of the word\\nrepresentations within the relevant context:\\nrc=1\\n|rc|X\\nj|wc\\nj∈rcwc\\nj (4)\\nBy incorporating the relevant context, our intention is to eliminate any disruptive or\\nunrelated details from the passage representation.\\nAnswer-Context Dependencies For convenience, let [r1,r2,r3]denote the representa-\\ntions acquired from Equation (4) for the answer passage p1≡c, the previous passage\\np2≡cprev, and the next passage p3≡cnext. To capture the relationships between\\nthese passages, we view each passage as a node in a fully-connected graph G= (V, E),\\nwhere V={pi}(1≤i≤3)is the node set and E={(pi, pj)}(1≤i, j≤3) is the\\nedge set. Our objective is to determine a weight αij∈(0,1)for each edge (pi, pj)that\\nreflects the dependency of pionpj. To accomplish this, we propose to leverage their\\nsemantic representations ri,rj, and transportation costs to the question dqpi,dqpjto\\nmeasure the dependency weight αijbetween the passages piandpj. Specifically, we first\\ncompute the score: uij=FFN DEP([ri⊙rj;dqpi;dqpj]), where ⊙is the element-wise\\nproduct, [; ]represents the concatenation operation, and FFN DEP is a feed-forward\\nnetwork. Subsequently, the weight αijfor the edge (pi, pj)is obtained through a softmax\\nfunction:\\nαij=exp(uij)PK\\nj′=1exp(uij′)(5)\\nThe derived weights {αij}are subsequently utilized to enrich the passage representations\\nthrough Llayers of a Graph Convolutional Network (GCN) [25]:\\nhl\\ni=ReLU (KX\\nj=1αijWlhl−1\\nj+bl) (6)\\nwhere Wl,blare learnable weight matrix and bias for the layer lof the GCN ( 1≤l≤L),\\nandh0\\ni≡riis the input representation for the passage pi. The output vectors hL\\ni≡hiat\\nthe last layer of the GCN serve as the final representations for the passages pi. Intuitively,\\nthe weights αijenable each passage to decide the amount of information it receives\\nfrom the other passages to improve its representation for the task. The representation h1\\nfor the answer passage p1≡cis finally sent to a feed-forward network with a sigmoid\\noutput function to estimate the correctness score pc∈(0,1)for the answer passage\\nc:pc=FFN DPR(h1). For training, we minimize the binary cross-entropy loss with\\nthe correctness scores pc. At inference time, consistent with previous research [13], we\\ninclude all answer passages for each question for ranking.\\n2.3 Generative LLM-based Question Answering Model\\nBackground on Text Generation Finetuning Text generation finetuning has become a\\ngeneral approach to solving different NLP tasks, where input and expected output of a', 'Finetunung_LLM.pdf'), (8, '8 Nguyen et al.\\ntask can be represented as source and target text respectively for a generative model to\\nlearn the task [44, 34, 29]. For example, a pretrained generative LLM such as BART [28]\\nand T5 [44] can be finetuned on sentiment analysis by taking a text statement (e.g., “I\\nreally like the story” ) as source text to generate a text label (i.e., “Positive” ,”Negative” ,\\n“Neutral” ) to indicate the sentiment of the statement. As the text generation resembles the\\npretraining tasks (e.g., predicting next words) for the generative LLMs, the formulation\\ncould facilitate the transfer learning to the target task. In addition, it enables the data\\naugmentation method where training data for a task may also be leveraged for another\\ntask if the two tasks both are convertable to the text generation format [31]. These\\nadvantages have led to significant performance improvements for many NLP tasks such\\nas event extraction [31], named entity recognition [66], and dependency parsing [29].\\nSimilar to other NLP tasks, the generative methods have been explored for improving QA\\nperformance [21, 49, 43, 19, 27, 18, 12, 37]. To avoid hallucination and improve factual\\naccuracy for the models, recent works on generative QA employ the retrieval-based\\nmethods such as GenQA [17] and WebGPT [38].\\nGenQA is introduced by Hsu et al. [17] for generating appropriate answers for user\\nquestions instead of simply choosing the best answer candidate. This expands the answer\\nretrieval pipeline with an additional generation stage to produce correct and satisfactory\\nanswers, even in cases where a highly ranked candidate is not acceptable or does not\\nprovide a natural response to the question. In particular, GenQA employs a pretrained\\ngenerative LLM to produce an answer by taking a given question and a list of answer\\ncandidates as input, sorted by a trained reranker system.\\nWebGPT is designed by OpenAI researchers [38] to tackle the problem of long-form\\nquestion-answering, which involves generating a paragraph-length answer to an open-\\nended question. Specifically, WebGPT uses the Microsoft Bing Web Search API to\\nretrieve relevant documents for a given question. The model then interacts with a text-\\nbased environment where it can take actions such as clicking on links or opening new\\nweb pages to locate relevant passages from which to generate answers.\\nOur Proposed Finetuning Strategy The main goal of a general text-generation model\\nis to produce an output text sequence y= [y1, y2, . . . , y T]based on a given input text\\nsequence x= [x1, x2, . . . , x S], where the lengths of the input and output sequences\\nare denoted by SandT, respectively. With a pretrained encoder-decoder LLM such as\\nBART [28] or T5 [44], we can compute the conditional probability of P(y|x)for training\\nthe model. At test time, the decoder merges the previous output and input text to create\\nthe current output. A decoding algorithm such as Greedy or Beam Search [63] can be\\nused to generate an output text with the highest likelihood. For QA, given a question\\nqand a retrieved web content c(e.g., top answer passages), previous works such as\\nGenQA and WebGPT are trained to take (q, c)for as the source sequence to produce a\\nresponse as the target sequence a=fθ(q, c), where fθdenotes the corresponding LLM\\nwith the parameters θ. As a result, fθmay merely learn to copy/synthesize information\\nfrom cto produce aifcoften contains necessary information for correctly answering\\nthe question qin training data. Relying solely on the retrieved content c, the model', 'Finetunung_LLM.pdf'), (9, 'Efficient Fine-tuning Large Language Models 9\\nmay fail to provide a correct answer for a given question if cis missing or contains\\nirrelevant/noisy information. In other words, performance of these retrieval-based QA\\nmodels are limited to an upper bound by the knowledge retriever.\\nDifferent from the previous works that follow a single-style “ a=fθ(q, c)” fine-\\ntuning strategy, we propose to employ a multi-style finetuning strategy, where both\\n“a=fθ(q, c)” and “ a=fθ(q)” are used to train the model. The latter intentionally\\nexcludes the external knowledge cfrom the input to encourage the model to retrieve its\\nown knowledge from the model parameters θ, which have been pretrained on massive\\nunlabeled text data [28, 44, 11, 56]. To combine the two finetuning styles, we propose\\nto finetune the LLM with “ a=fθ(q, c)”, and sequentially finetune the model with\\n“a=fθ(q)”. In this way, our model does not completely rely on the retrieval results to\\ngenerate answers for given questions. At test time, we use the “ a=fθ(q, c)” style to\\nmake predictions. The retrieved content cnow can be considered as a source of external\\nknowledge along with the pretrained knowledge contained in the model parameters θ\\nto generate an answer for the question. Under this perspective, we consider various QA\\ndatasets for each step in our finetuning process. We call such dataset collection OKQA\\nas they are publicly available and contains high-quality knowledge.\\nMS Marco QA NLG is a specialized version of the MS Marco dataset [39] that aims to\\nproduce natural language responses to user inquiries using web search result excerpts.\\nThis dataset includes 182Kqueries from Bing search logs, each is associated with top\\nten most relevant passages. A human annotator is then required to look at the passages\\nand synthesize an answer using the content of the passages that most accurately addresses\\nthe query.\\nSuper Natural Instructions (SNI) is a data collection proposed by [60]. The corpus\\nconsists of 1,616diverse NLP tasks and their expert-written instructions. In this work,\\nwe consider only question-answering tasks such as extractive QA with SQUAD [45]\\nand multiple-choice QA with MCTest [48]. For each task, we consider anything but a\\nquestion qprovided in the input as context c. Particularly, the context ccan be a passage,\\na fact, or a set of answer choices associated with the question. As a result, we obtain\\n180Kexamples for finetuning our model.\\nAnthropic is introduced by [1], containing conversations between a human and a com-\\nputer assistant. For each conversation, we consider a human question and the previous\\nquestion (if any) as the input sequence and the answer from the assistant as the output\\nsequence. As questions in a conversation are usually related to each other, the previous\\nquestion can be considered as a form of relevant context cfor clarifying the current\\nquestion q. Consequently, we obtain 280Kexamples for finetuning our model.\\nDense Passage Retrieval datasets, namely, WikiQA [69] and WDRASS [73] are also\\nused for finetuning our model. WikiQA is a collection of questions and answer candidates\\nthat have been manually annotated using Bing query logs on Wikipedia. WDRASS is\\na large-scale dataset of questions that are non-factoid in nature, such as questions that\\nbegin with “why” or “how”. The dataset contains around 64,000questions and over\\n800,000labeled passages that have been extracted from a total of 30Mdocuments. Each', 'Finetunung_LLM.pdf'), (10, '10 Nguyen et al.\\nquestion in such DPR datasets is associated with a set of answer candidates, in which\\nsome of the candidates are correct answers. As a question can have multiple correct\\nanswers, we select the longest answer as the output sequence for the question, which is\\nconsidered as the input sequence. This results in a set of 105Kexamples for finetuning\\nour model.\\nIn the end, the datasets where context is available for a question are employed in the\\nstep 1 of our finetuning process while the other datasets are used for further training the\\nmodel in the subsequent step. With a huge amount of various QA tasks, we expect this\\ncould teach the model to understand the nature of question answering and how to utilize\\nits own parametric knowledge (in case no context is provided) and external knowledge\\n(i.e., relevant context) to answer a given question.\\n3 Experiments\\n3.1 Benchmarking the Knowledge Retriever\\nExperimental Setup\\nDatasets We follow the previous work [13, 74] to conduct the evaluation. In particular,\\nwe use (i) WikiQA [69], consisting of questions from Bing query logs and manually\\nannotated answers from Wikipedia, and (ii) WDRASS [74], a large-scale web-based\\ndataset having factoid and non-factoid questions, to investigate our retrieval performance.\\nWe use the same train/dev/test splits used in previous work.\\nHyper-parameters and Tools In accordance with previous work, we use a small portion\\nof the WikiQA training data to tune hyper-parameters for our model and select the best\\nhyper-parameters for all the datasets [26]. We employ Adam optimizer to train the model\\nwith a learning rate of 1e-5and a batch size of 64. We set 400for the hidden vector\\nsizes for all the feed-forward networks, L= 2for the number of the GCN layers. We\\nuse Pytorch version 1.7.1 and Huggingface Transformers version 3.5.1 To implement\\nthe models. We use the NLTK library version 3.5 [4] to preprocess the data and remove\\nstopwords. The model performance is obtained over three runs with random seeds.\\nEvaluation Metrics We measure the model performance using the following standard\\nmetrics: Precision-at-1 (P@1) and Mean Average Precision (MAP) on the entire set of\\nanswer candidates for each question.\\nPerformance Comparison We compare our proposed model with TANDA [13], which\\nis the current state-of-the-art model. Table 2 shows the performance comparison between\\nthe models on two settings: i) using a non-finetuned RoBERTa-Base encoder, and ii) using\\na fine-tuned RoBERTa-Base encoder. The non-finetuned RoBERTa-Base is obtained\\nfrom [33] while the other is produced by fine-tuning TANDA on the ASNQ dataset\\n[13]. As can be seen from the table, all the models benefit from using the finetuned\\nRoBERTa-Base encoder. Across the two settings, our model outperforms the previous\\nmodels by large margins, demonstrating its effectiveness for the task.', 'Finetunung_LLM.pdf'), (11, 'Efficient Fine-tuning Large Language Models 11\\nModelWikiQA WDRASS\\nw/o ASNQ with ASNQ with ASNQ\\nP@1 MAP P@1 MAP P@1 MAP\\nTANDA 63.24* 75.00* 78.67* 86.74* 54.60 63.50\\nOurs 74.16 83.29 83.77 89.28 55.9 61.8\\nTable 2: Performance comparison on WikiQA and WDRASS, * indicates results reported\\nby [26].\\nIn Table 2, we show the performance of our proposed model compared to TANDA\\non the WDRASS test set. As we can see, our knowledge retriever significantly improves\\nthe performance for P@1 score, however, decreases the performance for MAP score.\\nWe attribute this to the fact that questions in WDRASS dataset usually have more than\\n1 correct answers for a single question while our model ranks the answer candidates\\nindividually. However, we note that the top-1 answer candidate is often the most helpful\\nfor the answering process.\\n3.2 Evaluation for Knowledge-Aware Answer Generation\\nExperimental Setup\\nDataset We acquire the evaluation data as follows. First, we randomly select 2,000\\nquestions from the MS MARCO QA NLG test set. For each question, we rank all the\\ncontext passages using our model trained on WDRASS to obtain the top 5 candidates.\\nWe then concatenate the question and candidates to form the input, which is used to\\ngenerate the predicted answer.\\nEvaluation Metrics We employ widely-used evaluation metrics, including ROUGE [30],\\nBLEU [40], and BERTScore [72], for assessing the quality of generated answers in\\ncomparison to human-written natural answers. These metrics are commonly applied to\\nstandard text generation tasks such as summarization [71], machine translation [57], and\\nanswer generation [43].\\nIt is important to note that these metrics have their own limitations; however, these\\ncan be mitigated by providing more and higher-quality reference texts [5]. In the context\\nof answer generation, we enhance the reliability of these measurements by employ-\\ning human-written answers as references. Specifically, annotators create the reference\\nanswers used in this benchmark after being provided with the candidate responses.\\nPerformance Comparison Table 3 presents a comparison of three different config-\\nurations of KARP with GenQA model in terms of BLEU, RougeL, and BERTScore\\nmetrics.\\nThe results demonstrate that all three KARP configurations outperform the GenQA\\nmodel across all evaluation metrics. The best-performing configuration (config 2)\\nachieves a BLEU score of 39.4, a RougeL score of 0.608, and a BERTScore of 0.752.\\nThese results indicate that KARP offers a significant improvement over the GenQA', 'Finetunung_LLM.pdf'), (12, '12 Nguyen et al.\\nModel BLEU RougeL BERTScore\\nGenQA [17] 14.6 0.518 0.698\\nKARP (config 1) 38.3 0.632 0.762\\nKARP (config 2) 39.4 0.608 0.752\\nKARP (config 3) 38.9 0.604 0.750\\nTable 3: Comparison of our three KARP models trained with different hyper-parameter\\nsettings to GenQA [17].\\nmodel in the context of answer generation, which we attribute to our specialized fine-\\ntuning strategy for QA.\\n3.3 End-to-end Evaluation for Knowledge-Aware Response Planning\\nIn this section, we evaluate KARP in an end-to-end industry-scale scenario.\\nExperimental Setup We outline the experimental setup to evaluate the end-to-end\\nperformance of KARP in a web-scale scenario, involving tens of millions of web\\ndocuments. The configuration allows us to study the scalability and effectiveness of our\\napproach in a real-world, large-scale setting.\\nWeb Document Collection We constructed a large collection of web data, comprising\\ndocuments and passages, to facilitate the development of knowledge retrieval for end-\\nto-end system evaluation. This resource enables us to assess the impact of our work\\nin an industry-scale ODQA setting. We selected English web documents from the top\\n5,000 domains, including Wikipedia, from Common Crawl’s 2019 and 2020 releases.\\nThe pages were split into passages following the DPR procedure [23], limiting passage\\nlength to 200 tokens while maintaining sentence boundaries. This produced a collection\\nof roughly 100 million documents and 130 million passages. From this, we built (i) a\\nstandard Lucene/Elasticsearch index and (ii) a neural-based DPR index [23].\\nWeb-scale Knowledge Retrieval For each question, we retrieved up to 1,000 docu-\\nments/passages using both indexes. We then rank the passages and applied our knowl-\\nedge retriever to select relevant passages. We used top K= 5 candidates as external\\nknowledge for a question.\\nQuestion Sampling We randomly selected 2,000 questions from WDRASS test set as it\\nshows to represent natural questions extracted from the Web. In addition, the questions\\nwere also manually labeled.\\nBaselines We employ GenQA [17] as our main baseline in this experiment. We compare\\nthe performance of our system obtained by our proposed fine-tuning strategy and the\\nstandard fine-tuning (i.e., combining all datasets for finetuning) in a data parity setting.', 'Finetunung_LLM.pdf'), (13, 'Efficient Fine-tuning Large Language Models 13\\nEvaluation Metrics We evaluate the performance of the end-to-end QA system using\\naccuracy metrics, i.e., the percentage of questions that were answered satisfactorily.\\nAdditionally, we define a correct answer as one that must not only be factually accurate,\\nbut also expressed in a natural and fluent manner. Answers that are too verbose or oddly\\nphrased are considered unsatisfactory.\\nPerformance Comparison The result show in the following table Table 4.\\nModel Accuracy\\nTANDA [13] baseline\\nGenQA [17] +2.20%\\nKARP →MS MARCO +6.20%\\nKARP →OKQA +7.40%\\nTable 4: Relative accuracy of different QA settings: TANDA [13], GenQA [17], and our\\nproposed frame work for KARP in two data configurations: MS MARCO (data parity)\\nand OKQA.\\nTable 4 presents the relative accuracy of different QA settings, including TANDA [13],\\nGenQA [17], and our proposed KARP with two data configurations: MS MARCO (data\\nparity) and (robust fine-tuning). From the table, we observe that GenQA outperforms\\nTANDA by 2.20%. Our proposed KARP model achieves even better results, with a\\n6.20% increase in accuracy when using the MS MARCO data configuration and a\\n7.40% increase in accuracy when using OKQAconfiguration. This demonstrates the\\neffectiveness of our proposed KARP model in various data settings.\\n4 Related Work\\nLarge Language Models (LLMs) LLMs have transformed NLP technologies with the\\nadvent of the Transformer architecture [57]. Two fundamental pre-training objectives,\\nMasked Language Modeling (MLM) and Causal Language Modeling (CLM), underpin\\nthe success of these models. MLM, introduced by BERT [10], predicts masked tokens in a\\nsentence using surrounding context, enabling LLMs to learn bidirectional representations\\nthat excel in various NLP tasks. In contrast, CLM, exemplified by GPT [41], predicts the\\nnext token in a sequence given its preceding context, showing remarkable success in text\\ngeneration and other downstream applications [42, 22, 43]. In this paper, we leverage the\\nCLM architecture for its language generation capabilities to enhance QA performance.\\nGeneral Question Answering using LLM A standard QA system consists of (i) a re-\\ntrieval engine that returns relevant knowledge and (ii) a model that generates a response\\naddressing the question, either through selection [52, 70, 13] or abstractive summariza-\\ntion of the top-selected answers [18, 12, 37]. In particular, recent summarization-based\\napproaches, e.g., GenQA [18, 12, 37], are highly susceptible to hallucination due to the', 'Finetunung_LLM.pdf'), (14, '14 Nguyen et al.\\nabsence of special treatment of irrelevant candidates, which commonly appear among\\nthe top-ranked options. As a result, the generated answer may seem plausible but could\\nbe factually incorrect [20, 76, 75, 64, 58, 54, 47, 46]. Even though its original goal is to\\ngenerate more natural answers, GenQA [18, 12, 37] can be considered as a method to\\nground LLMs for QA as it decodes an answer from the concatenation of both question\\nand answer candidates. This approach, however, requires good answer candidates and\\ncareful finetuning to reduce hallucinations.\\nWe propose, instead, a novel generation-based approach that leverages the emerging\\nlanguage reasoning capabilities of Large Language Models (LLMs) [41] to enhance\\nquality of generated answers. In particular, KARP is designed to mitigate the reliance\\non oracle data by making use of the context, such as all choices in multiple-choice QA,\\ninstead of a correct answer alone, i.e., the correct choice. The experiments demonstrated\\nthat our proposed framework for KARP is highly resilient to noisy input data, and bring\\nabout broader application across different QA tasks.\\nFine-tuning Strategies for LLMs Several fine-tuning strategies have been specifically\\nproposed for large language models (LLMs). These strategies can be broadly categorized\\ninto two groups: architecture-centric and data-centric. (i) Architecture-centric fine-tuning\\naims to improve the model’s robustness and adaptability by modifying hyper-parameters\\nacross layers. Gradual unfreezing [16] is one example, involving sequential fine-tuning\\nof model layers to prevent catastrophic forgetting and better adapt to downstream tasks.\\nLayer-wise learning rate decay [41] is another example, where different learning rates\\nare assigned to various layers to enable more refined adaptation to the target task.\\n(ii) Data-centric fine-tuning, on the other hand, concentrates on leveraging data from\\ndifferent sources or intermediate tasks to enhance model performance. Sequential fine-\\ntuning [14, 13] involves training the model on intermediate tasks before the final target\\ntask, improving its performance on the latter. Combining several related datasets for\\nmulti-task fine-tuning has also been shown to improve performance on the target task\\n[32]. Our work is related to data-centric fine-tuning. In particular, we propose a novel\\nstrategy specifically designed for the question answering context. By leveraging both\\nexternal knowledge and intrinsic parametric knowledge of LLMs, our approach aims to\\nenhance the quality of generated answers in QA tasks.\\n5 Conclusion\\nIn this paper, we presented a novel framework powered by large language models (LLMs)\\nfor KARP. To that end, we proposed an efficient fine-tuning strategy for KARP that\\nleverages (i) the emergent language reasoning abilities of LLMs and (ii) general question\\nanswering advances, including modelings and resources. Our experimental results show\\nthat KARP improves the state of the art in general QA tasks and outperforms vanilla\\nfine-tuning of LLMs in a dataset-parity setting. This research highlights the significance\\nof leveraging the intrinsic parametric knowledge of LLMs rather than relying solely on\\nconventional sequence-to-sequence fine-tuning, in order to improve their performance in\\nquestion answering tasks.', 'Finetunung_LLM.pdf'), (15, 'Efficient Fine-tuning Large Language Models 15\\nReferences\\n1.Bai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S.,\\nGanguli, D., Henighan, T., et al.: Training a helpful and harmless assistant with reinforcement\\nlearning from human feedback. arXiv preprint arXiv:2204.05862 (2022)\\n2.Bao, J., Duan, N., Yan, Z., Zhou, M., Zhao, T.: Constraint-based question answering with\\nknowledge graph. In: Proceedings of COLING 2016, the 26th international conference on\\ncomputational linguistics: technical papers. pp. 2503–2514 (2016)\\n3.Bao, J., Duan, N., Zhou, M., Zhao, T.: Knowledge-based question answering as machine\\ntranslation. In: Proceedings of the 52nd Annual Meeting of the Association for Computational\\nLinguistics (V olume 1: Long Papers). pp. 967–976 (2014)\\n4.Bird, S., Klein, E., Loper, E.: Natural language processing with Python: analyzing text with\\nthe natural language toolkit. ” O’Reilly Media, Inc.” (2009)\\n5.Callison-Burch, C., Osborne, M., Koehn, P.: Re-evaluating the role of Bleu in machine\\ntranslation research. In: 11th Conference of the European Chapter of the Association for\\nComputational Linguistics. pp. 249–256. Association for Computational Linguistics, Trento,\\nItaly (Apr 2006)\\n6.Chen, D., Fisch, A., Weston, J., Bordes, A.: Reading wikipedia to answer open-domain\\nquestions. arXiv preprint arXiv:1704.00051 (2017)\\n7.Chen, D., Yih, W.t.: Open-domain question answering. In: Proceedings of the 58th annual\\nmeeting of the association for computational linguistics: tutorial abstracts. pp. 34–37 (2020)\\n8.Clark, P., Etzioni, O., Khot, T., Sabharwal, A., Tafjord, O., Turney, P., Khashabi,\\nD.: Combining retrieval, statistics, and inference to answer elementary science ques-\\ntions. Proceedings of the AAAI Conference on Artificial Intelligence 30(1) (Mar 2016).\\nhttps://doi.org/10.1609/aaai.v30i1.10325\\n9.Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transport. Advances in\\nneural information processing systems 26(2013)\\n10.Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. In: Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, V olume 1 (Long and Short Papers). pp. 4171–4186. Association for Computational\\nLinguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423\\n11.FitzGerald, J.G.M., Ananthakrishnan, S., Arkoudas, K., Bernardi, D., Bhagia, A., Bovi, C.D.,\\nCao, J., CHADA, R., Chauhan, A., Chen, L., Dwarakanath, A., Dwivedi, S., Gojayev, T.,\\nGopalakrishnan, K., Gueudre, T., Hakkani-T ¨ur, D., Hamza, W., Hueser, J., Jose, K.M., Khan,\\nH., Liu, B., Lu, J., Manzotti, A., Natarajan, P., Owczarzak, K., Oz, G., Palumbo, E., Peris,\\nC., Prakash, C.S., Rawls, S., Rosenbaum, A., Shenoy, A., Soltan, S., Harakere, M., Tan,\\nL., Triefenbach, F., Wei, P., Yu, H., Zheng, S., Tur, G., Natarajan, P.: Alexa teacher model:\\nPretraining and distilling multi-billion-parameter encoders for natural language understanding\\nsystems. In: KDD 2022 (2022)\\n12.Gabburo, M., Koncel-Kedziorski, R., Garg, S., Soldaini, L., Moschitti, A.: Knowledge transfer\\nfrom answer ranking to answer generation. In: Proceedings of the 2022 Conference on Empir-\\nical Methods in Natural Language Processing. pp. 9481–9495. Association for Computational\\nLinguistics, Abu Dhabi, United Arab Emirates (Dec 2022)\\n13.Garg, S., Vu, T., Moschitti, A.: Tanda: Transfer and adapt pre-trained transformer models for\\nanswer sentence selection. Proceedings of the AAAI Conference on Artificial Intelligence\\n34(05), 7780–7788 (Apr 2020). https://doi.org/10.1609/aaai.v34i05.6282\\n14.Gururangan, S., Marasovi ´c, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., Smith, N.A.:\\nDon’t stop pretraining: Adapt language models to domains and tasks. In: Proceedings of the', 'Finetunung_LLM.pdf'), (16, '16 Nguyen et al.\\n58th Annual Meeting of the Association for Computational Linguistics. pp. 8342–8360. Asso-\\nciation for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-\\nmain.740\\n15.Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom,\\nP.: Teaching machines to read and comprehend. Advances in neural information processing\\nsystems 28(2015)\\n16.Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. In: Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers). pp. 328–339. Association for Computational Linguistics, Melbourne, Australia\\n(Jul 2018). https://doi.org/10.18653/v1/P18-1031\\n17.Hsu, C.C., Lind, E., Soldaini, L., Moschitti, A.: Answer generation for retrieval-based question\\nanswering systems. In: Findings of the Association for Computational Linguistics: ACL-\\nIJCNLP 2021. pp. 4276–4282. Association for Computational Linguistics, Online (Aug 2021).\\nhttps://doi.org/10.18653/v1/2021.findings-acl.374\\n18.Hsu, C.C., Lind, E., Soldaini, L., Moschitti, A.: Answer generation for retrieval-based question\\nanswering systems. In: ACL Findings 2021 (2021)\\n19.Izacard, G., Grave, E.: Leveraging passage retrieval with generative models for open domain\\nquestion answering. In: Proceedings of the 16th Conference of the European Chapter of\\nthe Association for Computational Linguistics: Main V olume. pp. 874–880. Association for\\nComputational Linguistics, Online (Apr 2021). https://doi.org/10.18653/v1/2021.eacl-main.74\\n20.Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E., Bang, Y .J., Madotto, A., Fung,\\nP.: Survey of hallucination in natural language generation. ACM Comput. Surv. 55(12) (mar\\n2023). https://doi.org/10.1145/3571730\\n21.Jiang, Z., Araki, J., Ding, H., Neubig, G.: Understanding and improving zero-shot multi-\\nhop reasoning in generative question answering. In: Proceedings of the 29th International\\nConference on Computational Linguistics. pp. 1765–1775. International Committee on Com-\\nputational Linguistics, Gyeongju, Republic of Korea (Oct 2022)\\n22.Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Rad-\\nford, A., Wu, J., Amodei, D.: Scaling laws for neural language models. arXiv preprint\\narXiv:2001.08361 (2020)\\n23.Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih, W.t.: Dense\\npassage retrieval for open-domain question answering. In: Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP). pp. 6769–6781. Association\\nfor Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-\\nmain.550\\n24.Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., Hajishirzi, H.: UNI-\\nFIEDQA: Crossing format boundaries with a single QA system. In: Findings of the As-\\nsociation for Computational Linguistics: EMNLP 2020. pp. 1896–1907. Association for\\nComputational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.findings-\\nemnlp.171\\n25.Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks.\\nIn: Proceedings of the 5th International Conference on Learning Representations (2017)\\n26.Lauriola, I., Moschitti, A.: Answer sentence selection using local and global context in\\ntransformer models. In: Advances in Information Retrieval: 43rd European Conference on\\nIR Research, ECIR 2021, Virtual Event, March 28–April 1, 2021, Proceedings, Part I. pp.\\n298–312. Springer (2021)\\n27.Lewis, M., Fan, A.: Generative question answering: Learning to answer the whole question.\\nIn: International Conference on Learning Representations (2019)\\n28.Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V .,\\nZettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for natural language', 'Finetunung_LLM.pdf'), (17, 'Efficient Fine-tuning Large Language Models 17\\ngeneration, translation, and comprehension. In: Proceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics. pp. 7871–7880. Association for Computational\\nLinguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.703\\n29.Lin, B., Yao, Z., Shi, J., Cao, S., Tang, B., Li, S., Luo, Y ., Li, J., Hou, L.: Dependency parsing\\nvia sequence generation. In: Findings of the Association for Computational Linguistics:\\nEMNLP 2022. pp. 7339–7353. Association for Computational Linguistics, Abu Dhabi, United\\nArab Emirates (Dec 2022)\\n30.Lin, C.Y .: ROUGE: A package for automatic evaluation of summaries. In: Text Summarization\\nBranches Out. pp. 74–81. Association for Computational Linguistics, Barcelona, Spain (Jul\\n2004)\\n31.Liu, J., Chen, Y ., Liu, K., Bi, W., Liu, X.: Event extraction as machine reading comprehension.\\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP). pp. 1641–1651. Association for Computational Linguistics, Online (Nov 2020).\\nhttps://doi.org/10.18653/v1/2020.emnlp-main.128\\n32.Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natural language under-\\nstanding. In: Proceedings of the 57th Annual Meeting of the Association for Computational\\nLinguistics. pp. 4487–4496. Association for Computational Linguistics, Florence, Italy (Jul\\n2019). https://doi.org/10.18653/v1/P19-1441\\n33.Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\\nL., Stoyanov, V .: Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 (2019)\\n34.Lu, Y ., Lin, H., Xu, J., Han, X., Tang, J., Li, A., Sun, L., Liao, M., Chen, S.: Text2Event:\\nControllable sequence-to-structure generation for end-to-end event extraction. In: Proceed-\\nings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing (V olume 1: Long\\nPapers). pp. 2795–2806. Association for Computational Linguistics, Online (Aug 2021).\\nhttps://doi.org/10.18653/v1/2021.acl-long.217\\n35.Maynez, J., Narayan, S., Bohnet, B., McDonald, R.: On faithfulness and factuality in ab-\\nstractive summarization. In: Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics. pp. 1906–1919. Association for Computational Linguistics,\\nOnline (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.173\\n36.Monge, G.: M ´emoire sur la th ´eorie des d ´eblais et des remblais. Mem. Math. Phys. Acad.\\nRoyale Sci. pp. 666–704 (1781)\\n37.Muller, B., Soldaini, L., Koncel-Kedziorski, R., Lind, E., Moschitti, A.: Cross-lingual open-\\ndomain question answering with answer sentence generation. In: Proceedings of the 2nd\\nConference of the Asia-Pacific Chapter of the Association for Computational Linguistics and\\nthe 12th International Joint Conference on Natural Language Processing (V olume 1: Long\\nPapers). pp. 337–353. Association for Computational Linguistics, Online only (Nov 2022)\\n38.Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju,\\nV ., Saunders, W., et al.: Webgpt: Browser-assisted question-answering with human feedback.\\narXiv preprint arXiv:2112.09332 (2021)\\n39.Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L.: Ms marco:\\nA human generated machine reading comprehension dataset (November 2016)\\n40.Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation\\nof machine translation. In: Proceedings of the 40th Annual Meeting of the Association\\nfor Computational Linguistics. pp. 311–318. Association for Computational Linguistics,\\nPhiladelphia, Pennsylvania, USA (Jul 2002). https://doi.org/10.3115/1073083.1073135\\n41.Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language under-\\nstanding by generative pre-training (2018)\\n42.Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are\\nunsupervised multitask learners (2019)', 'Finetunung_LLM.pdf'), (18, '18 Nguyen et al.\\n43.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu,\\nP.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach.\\nLearn. Res. 21(1) (jan 2020)\\n44.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu,\\nP.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The\\nJournal of Machine Learning Research 21(1), 5485–5551 (2020)\\n45.Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: SQuAD: 100,000+ questions for machine\\ncomprehension of text. In: Proceedings of the 2016 Conference on Empirical Methods in\\nNatural Language Processing. pp. 2383–2392. Association for Computational Linguistics,\\nAustin, Texas (Nov 2016). https://doi.org/10.18653/v1/D16-1264\\n46.Raunak, V ., Menezes, A., Junczys-Dowmunt, M.: The curious case of hallucinations in\\nneural machine translation. In: Proceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies. pp. 1172–1183. Association for Computational Linguistics, Online (Jun 2021).\\nhttps://doi.org/10.18653/v1/2021.naacl-main.92\\n47.Rebuffel, C., Roberti, M., Soulier, L., Scoutheeten, G., Cancelliere, R., Gallinari, P.: Control-\\nling hallucinations at word level in data-to-text generation. CoRR abs/2102.02810 (2021)\\n48.Richardson, M., Burges, C.J., Renshaw, E.: MCTest: A challenge dataset for the open-domain\\nmachine comprehension of text. In: Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing. pp. 193–203. Association for Computational Linguistics,\\nSeattle, Washington, USA (Oct 2013)\\n49.Roberts, A., Raffel, C., Shazeer, N.: How much knowledge can you pack into the parameters\\nof a language model? In: Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP). pp. 5418–5426. Association for Computational\\nLinguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.437\\n50.Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y ., Xu, J., Ott, M., Smith, E.M.,\\nBoureau, Y .L., Weston, J.: Recipes for building an open-domain chatbot. In: Proceedings of the\\n16th Conference of the European Chapter of the Association for Computational Linguistics:\\nMain V olume. pp. 300–325. Association for Computational Linguistics, Online (Apr 2021).\\nhttps://doi.org/10.18653/v1/2021.eacl-main.24\\n51.Saxena, A., Chakrabarti, S., Talukdar, P.: Question answering over temporal knowledge graphs.\\nIn: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Natural Language Processing (V olume 1:\\nLong Papers). pp. 6663–6676. Association for Computational Linguistics, Online (Aug 2021).\\nhttps://doi.org/10.18653/v1/2021.acl-long.520\\n52.Severyn, A., Moschitti, A.: Learning to rank short text pairs with convolutional deep neural\\nnetworks. In: Proceedings of the 38th international ACM SIGIR conference on research and\\ndevelopment in information retrieval. pp. 373–382 (2015)\\n53.Shuster, K., Poff, S., Chen, M., Kiela, D., Weston, J.: Retrieval augmentation reduces halluci-\\nnation in conversation. In: Findings of the Association for Computational Linguistics: EMNLP\\n2021. pp. 3784–3803. Association for Computational Linguistics, Punta Cana, Dominican\\nRepublic (Nov 2021). https://doi.org/10.18653/v1/2021.findings-emnlp.320\\n54.Shuster, K., Poff, S., Chen, M., Kiela, D., Weston, J.: Retrieval augmentation reduces halluci-\\nnation in conversation. In: Findings of the Association for Computational Linguistics: EMNLP\\n2021. pp. 3784–3803. Association for Computational Linguistics, Punta Cana, Dominican\\nRepublic (Nov 2021). https://doi.org/10.18653/v1/2021.findings-emnlp.320\\n55.Sinkhorn, R., Knopp, P.: Concerning nonnegative matrices and doubly stochastic matrices.\\nPacific Journal of Mathematics 21(2), 343–348 (1967)\\n56.Soltan, S., Ananthakrishnan, S., FitzGerald, J.G.M., Gupta, R., Hamza, W., Khan, H., Peris, C.,\\nRawls, S., Rosenbaum, A., Rumshisky, A., Prakash, C.S., Sridhar, M., Triefenbach, F., Verma,', 'Finetunung_LLM.pdf'), (19, 'Efficient Fine-tuning Large Language Models 19\\nA., Tur, G., Natarajan, P.: Alexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model. arXiv (2022)\\n57.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u.,\\nPolosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V ., Bengio, S., Wallach, H.,\\nFergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing\\nSystems. vol. 30. Curran Associates, Inc. (2017)\\n58. Wang, C., Sennrich, R.: On exposure bias, hallucination and domain shift in neural machine\\ntranslation. In: Proceedings of the 58th Annual Meeting of the Association for Computational\\nLinguistics. pp. 3544–3552. Association for Computational Linguistics, Online (Jul 2020).\\nhttps://doi.org/10.18653/v1/2020.acl-main.326\\n59.Wang, W., Yang, N., Wei, F., Chang, B., Zhou, M.: Gated self-matching networks for reading\\ncomprehension and question answering. In: Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (V olume 1: Long Papers). pp. 189–198 (2017)\\n60.Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y ., Mirzaei, A., Naik, A., Ashok, A.,\\nDhanasekaran, A.S., Arunkumar, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H., Purohit,\\nI., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Pal, K.K., Patel, M., Moradshahi, M.,\\nParmar, M., Purohit, M., Varshney, N., Kaza, P.R., Verma, P., Puri, R.S., Karia, R., Doshi, S.,\\nSampat, S.K., Mishra, S., Reddy A, S., Patro, S., Dixit, T., Shen, X.: Super-NaturalInstructions:\\nGeneralization via declarative instructions on 1600+ NLP tasks. In: Proceedings of the\\n2022 Conference on Empirical Methods in Natural Language Processing. pp. 5085–5109.\\nAssociation for Computational Linguistics, Abu Dhabi, United Arab Emirates (Dec 2022)\\n61.Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma,\\nM., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus,\\nW.: Emergent abilities of large language models. Transactions on Machine Learning Research\\n(2022), survey Certification\\n62.Weston, J., Bordes, A., Chopra, S., Rush, A.M., Van Merri ¨enboer, B., Joulin, A., Mikolov,\\nT.: Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint\\narXiv:1502.05698 (2015)\\n63.Wiseman, S., Rush, A.M.: Sequence-to-sequence learning as beam-search optimization. In:\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Process-\\ning. pp. 1296–1306. Association for Computational Linguistics, Austin, Texas (Nov 2016).\\nhttps://doi.org/10.18653/v1/D16-1137\\n64.Xiao, Y ., Wang, W.Y .: On hallucination and predictive uncertainty in conditional language\\ngeneration. In: Proceedings of the 16th Conference of the European Chapter of the Association\\nfor Computational Linguistics: Main V olume. pp. 2734–2744. Association for Computational\\nLinguistics, Online (Apr 2021). https://doi.org/10.18653/v1/2021.eacl-main.236\\n65.Xu, J., Wang, Y ., Tang, D., Duan, N., Yang, P., Zeng, Q., Zhou, M., Sun, X.: Asking clar-\\nification questions in knowledge-based question answering. In: Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1618–1629 (2019)\\n66.Yan, H., Gui, T., Dai, J., Guo, Q., Zhang, Z., Qiu, X.: A unified generative framework for\\nvarious NER subtasks. In: Proceedings of the 59th Annual Meeting of the Association for\\nComputational Linguistics and the 11th International Joint Conference on Natural Language\\nProcessing (V olume 1: Long Papers). pp. 5808–5822. Association for Computational Linguis-\\ntics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.451\\n67.Yang, W., Xie, Y ., Lin, A., Li, X., Tan, L., Xiong, K., Li, M., Lin, J.: End-to-end open-\\ndomain question answering with BERTserini. In: Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics (Demonstrations).\\npp. 72–77. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019).\\nhttps://doi.org/10.18653/v1/N19-4013', 'Finetunung_LLM.pdf'), (20, '20 Nguyen et al.\\n68.Yang, Y ., Yih, W.t., Meek, C.: Wikiqa: A challenge dataset for open-domain question an-\\nswering. In: Proceedings of the 2015 conference on empirical methods in natural language\\nprocessing. pp. 2013–2018 (2015)\\n69.Yang, Y ., Yih, W.t., Meek, C.: WikiQA: A challenge dataset for open-domain question\\nanswering. In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language\\nProcessing. pp. 2013–2018. Association for Computational Linguistics, Lisbon, Portugal (Sep\\n2015)\\n70.Yoon, S., Dernoncourt, F., Kim, D.S., Bui, T., Jung, K.: A compare-aggregate model with latent\\nclustering for answer selection. In: Proceedings of the 28th ACM International Conference on\\nInformation and Knowledge Management. pp. 2093–2096 (2019)\\n71.Zhang, J., Zhao, Y ., Saleh, M., Liu, P.: PEGASUS: Pre-training with extracted gap-sentences\\nfor abstractive summarization. In: III, H.D., Singh, A. (eds.) Proceedings of the 37th In-\\nternational Conference on Machine Learning. Proceedings of Machine Learning Research,\\nvol. 119, pp. 11328–11339. PMLR (13–18 Jul 2020)\\n72.Zhang*, T., Kishore*, V ., Wu*, F., Weinberger, K.Q., Artzi, Y .: Bertscore: Evaluating text\\ngeneration with bert. In: International Conference on Learning Representations (2020)\\n73.Zhang, Z., Vu, T., Gandhi, S., Chadha, A., Moschitti, A.: Wdrass: A web-scale dataset\\nfor document retrieval and answer sentence selection. In: Proceedings of the 31st ACM\\nInternational Conference on Information & Knowledge Management. pp. 4707–4711 (2022)\\n74.Zhang, Z., Vu, T., Gandhi, S., Chadha, A., Moschitti, A.: Wdrass: A web-scale dataset\\nfor document retrieval and answer sentence selection. In: Proceedings of the 31st ACM\\nInternational Conference on Information and Knowledge Management. p. 4707–4711. CIKM\\n’22, Association for Computing Machinery (2022)\\n75.Zhao, Z., Cohen, S.B., Webber, B.: Reducing quantity hallucinations in abstractive sum-\\nmarization. In: Findings of the Association for Computational Linguistics: EMNLP\\n2020. pp. 2237–2249. Association for Computational Linguistics, Online (Nov 2020).\\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.203\\n76.Zhou, C., Neubig, G., Gu, J., Diab, M., Guzm ´an, F., Zettlemoyer, L., Ghazvininejad, M.:\\nDetecting hallucinated content in conditional neural sequence generation. In: Findings of the\\nAssociation for Computational Linguistics: ACL-IJCNLP 2021. pp. 1393–1404. Association\\nfor Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.findings-\\nacl.120', 'Finetunung_LLM.pdf'), (1, ' Karan Singh, Assistant Professor of Operations Research \\nPrinciples of Generative AI A Technical Introduction Generative artificial intelligence (GenAI) tools are an emerging class of new-age artificial intelligence algorithms capable of producing novel content — in varied formats such as text, audio, video, pictures, and code — based on user prompts. Recent advances in machine learning (ML), massive datasets, and substantial increases in computing power have propelled such tools to human-level performance on academic and professional benchmarks, 1comparable to the ninetieth percentile on the SAT and the bar exam. This rapid progress has led many to to believe that the metamorphosis of these technologies 2from research-grade demos to accessible and easy-to-use production-grade goods and services carries the potential to supercharge business processes and operations while enabling entirely new deliverables heretofore rendered infeasible by economic or technological factors. It took OpenAI’s ChatGPT, a conversational web app based on a generative (multimodal) language model, about five days to reach one million users (compared to 2.5 months for 3Instagram). On the business side, the Economist reports that the number of jobs mentioning AI-related skills quadrupled from 2022 to 2023. This enthusiasm has not gone unmet by investors. Generative AI startups reportedly raised 600% more capital in 2022 than in 2020.   4 \\n 1\\nFigure 1: A taxonomy of GenAI-related disciplines.', 'genai-principles.pdf'), (2, ' Karan Singh, Assistant Professor of Operations Research \\nPurpose and Scope  What are these new-era AI technologies? How do they function? What principles do they operate on? What makes them different than already-hyped-up conventional machine learning (ML) models? For what tasks is this class of technology most impactful? What future advances might one look forward to? These are the questions this report attempts to shed some light on. The report will also tease out how this understanding foundationally informs the best uses (and misuses) of GenAI in applied contexts. A word of disclaimer: this gradient of topics also means that, while the initial sections deal with factual, if somewhat simplified, nuts-and-bolt workings of such models, the later sections delve into hopefully reasonable, but in a manner that only time may attest to, extrapolations and speculations, as necessitated by the developing nature of this technology and its current phase in the technology adoption cycle. While generative AI models come in many different shapes, utilizing varied statistical and computational techniques to target various modalities, ranging from code and text to audio and video, this report focuses almost exclusively on large language models (LLMs) capable of generating novel text from textual prompts. This choice is partly due to the substantial lead LLMs have in driving the overall usage of generative AI models and partly due to the centrality 5of language in formulating and addressing commonplace information-processing tasks. That said, image- and code-based GenAI models have already witnessed successful commercial product deployment, for example, by Adobe for creating visual content and by Github as a programming assistance tool.   \\n 2\\nFigure 2: An image-based GenAI model, Midjourney’s response to the prompt — “Businessman in Tokyo amidst rush hour, his gaze fixed ahead, surrounded by a sea of black umbrellas.”\\nFigure 3: Based on a code-based GenAI model, OpenAI Codex, Github Copilot is a commercial tool that can generate functional code from specifications given as natural language. Reportedly, as of June 2023, it served over a million users. ', 'genai-principles.pdf'), (3, ' Karan Singh, Assistant Professor of Operations Research \\nA Quick First Introduction to Language Models At its core, a language model implements a simple functionality— to predict the next word (or token) given a context window specifying preceding words. More precisely, given a context window, a language model outputs a probability distribution over all possible words in its vocabulary, indicating the probability with which each possible word follows the given list of words. Upon sampling a guess of the next word from the said distribution, the language model 6incrementally repeats this ostensibly primitive step to produce a more extensive body of text.     \\nWe make two observations here: 1.Completions are random. The predicted completion, given a context window, is not deterministic. Sampling the next word in each step from the output distribution introduces enough randomness to permit that the predicted completions could be meaningfully different on every fresh run. This stochasticity is why ChatGPT, for instance, can offer varied answers for the same prompt across successive runs. Replacing the sampling step with choosing (greedily) the most likely immediate word is known to degrade the quality of the produced text. The randomness in responses is also desirable from a user  3\\nFigure 4: A probabilistic model predicting the next word coupled with sampling can produce larger bodies of text.', 'genai-principles.pdf'), (4, ' Karan Singh, Assistant Professor of Operations Research \\nperspective in getting varied responses. From the deployer’s perspective, this optionally allows the model to gather user feedback regarding the quality of seemingly plausible responses. This choice partly also contributes to hallucination in language models. 2.Initial prompt matters. Language models are conditional probabilistic models. They produce a completion conditioned on the initial set of words. In this way, the initial context window, termed prompt, matters crucially to the produced completion. One hallmark of modern language models is that they keep track of the initial prompt even when generating large bodies of text, unlike the earlier generation of models, thus producing more coherent responses. Artful and cleverly crafted prompts can significantly improve the quality and utility of the synthesized text. Prompt engineering, for example, practices 7that encourage the language model to solve a problem by decomposing it into intermediate subproblems, has been known to improve the performance on logical reasoning tasks. Contextualizing LLMs in terms of Recent AI Advances Although we describe the text generation procedure above, many questions still need to be addressed: How do language models function internally? How are the output probabilities for the next word determined? What goes into creating (and indeed using) a language model? How are language models different from more traditional predictive models if all they do is predict the next token? We address these questions indirectly in the present section by taking a tour of the essential significant developments in machine learning and artificial intelligence that have occurred in the last decade and have fueled the creation of modern large language models. Classical Machine Learning as Prediction Machines We start with the most well-understood subset of machine learning techniques: supervised learning. The central objective in supervised learning is to produce a prediction rule that predicts well on unseen data, given enough labeled examples. For example, consider predicting house prices from the square footage in a given zip code. Instead of creating a hand-crafted prediction rule, the machine learning methodology advocates for choosing a prediction rule from an expressive but non-exhaustive class of rules, such as linear predictors, that provides the best fit on an existing collection of size-price examples. The statistically well-substantiated leap of faith here is that we expect (or at least hope) that a parsimonious prediction rule that predicts well on collected data, for which we know the correct answers, continues to maintain its predictive edge on unseen data, where answers or prices are unknown. Such a predictive methodology benefits from an abundance of labeled examples, hoping that a prediction rule learned from more examples is more robust in that its superior predictive performance on seen data is less ascribable to chance alone. Another example of a supervised learning task is to separate spam from non-spam mail, given the text in email messages. Again, having more examples of spam and non-spam emails is helpful to a supervised learning algorithm.  4', 'genai-principles.pdf'), (5, ' Karan Singh, Assistant Professor of Operations Research \\n Characteristics common to both language models and supervised learning: 1.Predicting Well is the Yardstick. A prediction rule is good as long as it makes reasonable predictions on average. Compared to more ambitious sub-disciplines in statistics, any statements about causality, p-values, and recovering latent structure are absent. We are similarly impervious to such considerations in language models. Such simplicity of goals enables very flexible prediction rules in machine learning. Although seeming modest in its aim, the art of machine learning has long been to cast as many disparate problems as questions about prediction as possible. Predicting house prices from square footage is a regular regression task. But, for reverse image captioning, is “predicting” a (high-dimensional) image given a few words a reasonable or well-defined classification task? Yet, this is how machine learning algorithms function. 2.Model Agnosticism. Supervised learning algorithms realize the adage that all models are wrong, but some are useful. For example, when building the price predictor above, a data scientist does not believe that the genuine relationship between prices and area is linear or well-specified. Similarly, when using neural networks to predict the next word in language models, we don’t believe that this is how Shakespeare must have employed a neural network to compose his texts. Yet, there are crucial differences:  5\\nFigure 5: Predicting house prices from square footage. Pictured is a linear regression, an example of a supervised learning algorithm that uses extant data to learn a linear predictor.', 'genai-principles.pdf'), (6, ' Karan Singh, Assistant Professor of Operations Research \\n1.Fidelity of Seen Data vs. Unseen Data. Classical supervised learning operates on the assumption that seen data must be representative of unseen data in a particular sense, namely that any fixed example is equally likely to be in the seen or unseen bucket. In the absence of temporal effects, this is reasonable for house prices. More generally, supervised learning requires a well-curated dataset that is closely aligned with the prediction task at hand. But, as we will see, language models are trained on vast corpora of somewhat ruthlessly collected texts from the internet. Yet, completing a random partial sentence from the internet is presumably not what businesses using language models care about. Deep Learning as Automated Representation Learning Although useful for panel or tabular data, pre-deep-learning-era supervised algorithms struggled to predict well when presented with visual or auditory inputs. Although the promise of machine learning is predicated on the automation of learning, in practice, supervised learning algorithms require carefully crafted representations of input data in which operations like additions and multiplications, for example, for linear regression, were semantically relevant. Decades of painstaking research in signal processing and computer vision had resulted in domain-specific hand-crafted representations, each useful for a specific modality (images, audio, or video). The predictive performance of ML algorithms was limited by how good such representations were. \\n 6\\nFigure 6: A typical deep neural network for recognizing faces. Each successive layer progressively learns higher-level representations (from edges to contours to faces). ', 'genai-principles.pdf'), (7, ' Karan Singh, Assistant Professor of Operations Research \\nThe revolution in deep learning was to automate the process of representation learning itself. Deep learning uses neural networks with multiple layers, each layer incrementally converting the data into a more manageable form, all to make better predictions. This form of automated hierarchical representation learning heralded a decade of tremendous progress in image and speech recognition and machine translation, starting with the breakthrough work of Krizhevsky, Sutskever, and Hinton in 2012 on the Imagenet challenge. Taking advantage of GPUs (a form 8of shared-memory parallel computing) and the availability of a large public dataset, this seminal work slashed the error rate for image recognition by a substantial multiple. Parallel gains were later realized using similar deep neural network architectures in speech recognition and other machine learning domains. In this sense, the advances deep learning enabled were (relatively) domain agnostic. Although deep neural networks are data-hungry in that they require a substantially large dataset to start predicting well, they also successfully realize a long-promised advantage of neural networks. This factor is crucial to the practice of modern-day machine learning. In the process of hierarchically learning representations, deep nets learn task- (or label--) agnostic features of the dataset in the lower layers, while higher layers closer to the output account for task-specific representations. This permits us to (a) train a deep net to separate images of cats and dogs on a large dataset and (b) subsequently build a shallow (even linear) performant neural net that uses the lower layers of the former to craft useful representations to classify images of zebra and giraffes. Step A is often called pre-training, and step B is referred to as supervised fine-tuning. This manner of amortizing the learning across tasks that are not individually data-rich is central to language models. Word Embeddings and Contrastive Learning While the progress of deep learning in speech and audio was made possible by the availability of large crowd-labeled datasets (with 10s of millions of annotated images), such large high-quality datasets were absent in the textual domain, despite a plethora of unlabelled data in the form of books, Wikipedia articles, and articles on the internet. Could a machine learning algorithm make use of the cheap, unlabelled data instead? In computational linguistics, the distributional hypothesis codifies an appealing and intuitive idea that similar words occur in similar contexts. In 2013, inspired by this observation, Mikolov et al 9trained a neural network, termed Word2Vec, to predict randomly selected words in a text corpus given neighboring words for each. Note that this step doesn’t require any need human annotators. They observed that the 300-dimensional vector representations the neural net learned for words had excellent linear algebraic properties that transparently reflected the underlying semantics. For example, one obtained Queen when queried for the word with the vector closest to King - Man + Woman. Thus, each vector dimension captured some abstract semantic degree of freedom. These representations were also valuable for natural classification tasks with limited data, such as sentiment classification, given a small number of examples.  7', 'genai-principles.pdf'), (8, ' Karan Singh, Assistant Professor of Operations Research \\n The approach of creating auxiliary labeling tasks for free from unlabelled data to learn semantically relevant representation is called contrastive learning and has proved helpful in other domains, too. For example, given a set of unlabelled images, a classifier trained to recognize random crops from the same image as a positive match and those from distinct images as a negative match (pre-training step) learns representations useful for supervised fine-tuning on genuine classification tasks downstream. Transformers mollify the Optimization Landscape While word embeddings serve as proof that textual semantic regularities can be assessed without labeled data, substantive language processing tasks need an algorithmic implementation of the concept of memory to capture relationships between words that are positionally far apart. For example, a common motif in stories is that the next act derives from some event that occurred a while ago.  \\n 8\\nFigure 7: Vector space representations of words exhibit linear algebraic relationships between semantic units and can be used to answer analogy questions, e.g., son - father + mother = daughter.\\nFigure 8: RNNs capture memory effects by sequentially processing information.', 'genai-principles.pdf'), (9, ' Karan Singh, Assistant Professor of Operations Research \\nThe first generation of neural networks that captured the notion of memory were Recurrent Neural Networks (RNNs), by sequentially processing a piece of text one word at a time while updating an internal state to maintain continuity, a proxy for memory. Unfortunately, optimizing such recurrent neural nets to find one that best fits a given dataset proved extra-ordinarily error-prone and challenging. In 2017, Vaswani et al introduced a different neural network architecture, termed transformer, 10that could efficiently capture long-range relations between tokens compactly (non-sequentially) by processing the entire surrounding context window at once while remaining amenable to gradient-based optimization. The introduction of transformers spurred a line of research on language models, culminating in training models with an increasingly higher number of parameters trained on ever larger datasets. For example, GPT2 (Generative Pre-trained Transformer 2), released in 2019, is a 1.5 billion parameter model trained on 40 GB of data, while GPT3, released in 2020, is a 175 billion parameter model trained on 570 GB of text data. While larger models resulted in better performance, the open-market cost for training these enormous models was estimated to be tens of millions of dollars.  \\nGeneral-Purpose Language Models: Supervised Fine-tuning & GPT3 The general paradigm brought about by contrastive learning was first to learn a large model on auxiliary tasks created using an unlabelled dataset (the pre-training step) and subsequently to use these learned representations in a downstream supervised learning task given a few task-specific labeled examples (the supervised fine-tuning step). While broadly useful and practical, supervised fine-tuning requires replicas of the baseline pre-trained model for each downstream  9\\nFigure 9: The LLM arms race with exponentially increasing parameter counts. (Credit: HuggingFace)', 'genai-principles.pdf'), (10, ' Karan Singh, Assistant Professor of Operations Research \\ntask; further, the large size of language models makes running even a few steps of gradient-based iterative optimization for supervised learning prohibitive except on computationally expensive hardware setups. The paper describing the architecture of the GPT3 model presents a far cheaper and more 11convenient way of repurposing pre-trained language models for specific downstream tasks, namely, by specifying a few labeled examples in the prompt before asking for a label or response for unseen data. This mode of inference, in-context learning, does not require computationally expensive adjustments to the weights or parameters of an LLM and instead treats the entire downstream supervised task as a prompt for the language model to complete. This makes LLMs very attractive for end-users, who no longer have to create copies of the large model to customize, nor do they have to run a sophisticated optimization procedure to adjust parameters; each downstream task, in effect, becomes a conversation. While fine-tuning may still result in additional performance gains over in-context learning for some tasks in exchange for a massive increase in computational load, a crucial advance of GPT3 is that this substantially lowers this gap, democratizing the use (although not the training) of LLMs.  \\n 10\\nFigure 10: An illustration of in-context learning. GPT4 figures out the correct pattern that the answer is the first number + reverse of the second, given two examples.', 'genai-principles.pdf'), (11, ' Karan Singh, Assistant Professor of Operations Research \\nTowards Conversational AI: Learning from Human Feedback While GPT3-like models happen to be good at conversation-centered tasks, they are not explicitly trained or incentivized to follow instructions. OpenAI’s InstructGPT model post pre-12training aligns the model to follow the users’ instructions by fine-tuning the model to mimic labeled demonstrations of the desired behavior (via supervised learning) and highly-ranked responses to prompts as collected using human feedback (via reinforcement learning).  \\nThe Future: Foundation Models Given the success of language models, there has been increased interest in the possibility of recreating the magic of LLMs in other domains. Such models, generically termed foundation models, attempt to amortize the cost of limited-data downstream tasks by pre-training on large corpora of broadly related tasks or unlabelled datasets. For example, one might be able to repurpose the LLM paradigm to train a generalist robot or decision-making agent that learns from supply chain operations across all industries. Conclusion This report contextualizes large-language models within the more extensive machine learning and artificial intelligence landscape by training the origins of the principal ideas that fuel today’s large language models. By bringing out their essential characteristics and differences against traditional modes of machine learning, we hope that a user of such models can be better  11\\nFigure 11: While GPT3 performs text completion by guessing the most plausible completion, InstructGPT has been explicitly trained to follow instructions. (Credit: OpenAI’s web report)', 'genai-principles.pdf'), (12, ' Karan Singh, Assistant Professor of Operations Research \\ninformed of the underlying tradeoffs such models induce, e.g., the performance-resource tradeoffs between fine-tuning and in-context learning. Endnotes  See the ﬁrst table on OpenAI’s announcement for an overview of GPT4’s performance on other academic, 1professional and programming exams. The quoted nineMeth percenMle performance on the bar exam was assessed by Katz et al, but others have raised concerns.  See quotes by industry and research leaders here.2 See iniMal consumer adopMon staMsMcs for ChatGPT here and here.3 See this reporMng for investments in GenAI.4 See current and project user bases for GenAI here.5 When producing text, rather than sampling the next word incrementally, a more systemaMc search operaMon 6termed Beam Search, coined by Raj Reddy at CMU, oXen yields beYer results. Structuring iniMal text to elicit useful outputs from GenAI model is called prompt engineering.7 See the full Krizhevshy, Sutskever, Hinton paper here.8 See the Word2Vec paper here.9 See the paper that introduced Transformers here.10 See the GPT3 paper here.11 See the instruct GPT paper here.12\\n 12', 'genai-principles.pdf')]\n"
     ]
    }
   ],
   "source": [
    "print(text_by_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ad563c4-066e-4183-968c-1c74cc34e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "CHUNK_SIZE=10000\n",
    "OVERLAP_SIZE=1000\n",
    "def chunk_text_with_pages(text_by_page, chunk_size=CHUNK_SIZE, overlap_size=OVERLAP_SIZE):\n",
    "    chunks_with_metadata = []\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\",\"\\n\"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = chunk_size,  # size of each chunk created\n",
    "    chunk_overlap  = overlap_size,  # size of  overlap between chunks in order to maintain the context\n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    "    )\n",
    "    for page_number, text,pdf_file, in text_by_page:\n",
    "        chunks = splitter.split_text(text)\n",
    "        for chunk in chunks:\n",
    "            chunks_with_metadata.append({\"chunk\": chunk, \"page_number\": page_number, \"file_name\": pdf_file})\n",
    "    return chunks_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b843ef1f-7e6a-4b35-b93c-8ef9def1dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text using LangChain\n",
    "chunks_with_metadata = chunk_text_with_pages(text_by_page, CHUNK_SIZE, OVERLAP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31bf50de-8072-41a6-b626-e4999d80f7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': \"Resolving challenges in a Retrieval-Augmented Generation (RAG) pipeline at the enterprise level involves a combination of advanced technical implementations, infrastructure optimization, and organizational practices. Here’s how each challenge can be addressed:\\n\\n1. Retriever Efficiency\\nChallenge: Retrieving relevant documents from large corpora can be computationally expensive and time-intensive.\\n\\nSolutions:\\nEfficient Indexing:\\nUse high-performance vector search engines like FAISS, Pinecone, or Weaviate to create optimized indexes.\\nEmploy hierarchical indexing or partitioning to speed up retrieval.\\nSparse + Dense Fusion:\\nCombine traditional methods like BM25 with dense vector retrieval for a hybrid approach that balances efficiency and accuracy.\\nPre-filtering:\\nApply metadata-based filtering (e.g., categories, tags) to reduce the search space before dense similarity search.\\nAsynchronous Updates:\\nIncrementally update the knowledge base with new documents using asynchronous indexing to minimize downtime.\\nBatch Query Processing:\\nFor high-throughput systems, batch queries to reduce redundant computations and improve resource utilization.\\n2. Knowledge Grounding\\nChallenge: Ensuring the generated response is grounded in the retrieved context and not hallucinated.\\n\\nSolutions:\\nFaithful Generation Techniques:\\nFine-tune the generative model on domain-specific datasets to improve its ability to stick to provided contexts.\\nUse RAG-Token models to ensure the generator dynamically attends to tokens from all retrieved documents.\\nContent Verification:\\nImplement post-generation verification by comparing the generated output against the retrieved documents using natural language inference (NLI) models.\\nRetriever-Generator Interaction:\\nUse iterative refinement where the generator highlights uncertainties, and the retriever fetches more specific documents in response.\\nDomain-Specific Constraints:\\nAdd constraints during generation, such as limiting outputs to specific vocabularies or document phrases.\\n3. Document Relevance\\nChallenge: The retriever might fetch irrelevant or partially relevant documents.\\n\\nSolutions:\\nRetriever Fine-tuning:\\nFine-tune the retriever on enterprise-specific datasets to align retrieval with the organization's domain and context.\\nRe-ranking with Advanced Models:\\nUse transformer-based ranking models (e.g., BERT-based cross-encoders) to re-rank retrieved documents based on query relevance.\\nFeedback Loops:\\nCollect user feedback to refine retriever behavior iteratively. For instance, capture user corrections or preferences and incorporate them into retraining.\\nContextual Filtering:\\nFilter results based on query intent using additional layers of intent classification and document tagging.\\n4. Scalability\\nChallenge: Scaling RAG for very large corpora and handling high traffic.\\n\\nSolutions:\\nDistributed Infrastructure:\\nUse distributed systems like Apache Spark, Elasticsearch, or cloud-based solutions like AWS OpenSearch to handle large-scale indexing and retrieval.\\nShard and Replicate:\\nShard the knowledge base into smaller, manageable segments and replicate for high availability.\\nDynamic Scaling:\\nImplement auto-scaling for retrieval and generation pipelines to handle traffic spikes without compromising performance.\\nMemory Optimization:\\nUse memory-efficient models and techniques like quantization for retriever and generator embeddings to reduce computational overhead.\\n5. Latency\\nChallenge: Combining retrieval and generation can introduce delays, especially in real-time applications.\\n\\nSolutions:\\nCaching:\\nCache frequently accessed query results to reduce retrieval time.\\nCache common query-document pairs and precompute embeddings for high-frequency queries.\\nParallel Processing:\\nExecute retrieval and pre-processing steps in parallel to reduce total pipeline time.\\nModel Optimization:\\nUse lightweight models like DistilBERT or ALBERT for retrieval and generation.\\nApply techniques like pruning and knowledge distillation to reduce model size while maintaining accuracy.\\nLatency-Budgeted Design:\\nFor real-time applications, limit the number of documents retrieved (e.g., top-k) and keep context inputs to manageable sizes.\\n6. Knowledge Updates\\nChallenge: Incorporating newly available information into the pipeline in near real-time.\\n\\nSolutions:\\nIncremental Indexing:\\nRegularly add new documents to the index without rebuilding it entirely using incremental indexing features of vector databases.\\nEvent-Driven Updates:\\nAutomate knowledge updates using event-driven pipelines (e.g., Apache Kafka or AWS Lambda) to process new data streams in real-time.\\nDomain-Specific Knowledge Bases:\\nMaintain separate, frequently updated knowledge bases for critical domains and integrate them dynamically into the pipeline.\\n7. Cost Management\\nChallenge: RAG pipelines can be expensive due to compute and storage requirements.\\n\\nSolutions:\\nCloud-Based Solutions:\\nUse pay-as-you-go cloud services like AWS, Google Cloud, or Azure for hosting infrastructure.\\nModel Compression:\\nReduce compute costs by employing techniques like model quantization, pruning, or distillation.\\nCost-Efficient Retrieval:\\nUse sparse retrievers like BM25 for queries where exact matches are likely, reserving dense retrieval for complex queries.\\n8. Security and Privacy\\nChallenge: Ensuring sensitive enterprise data is secure in retrieval and generation processes.\\n\\nSolutions:\\nAccess Control:\\nImplement role-based access control (RBAC) to restrict access to sensitive data.\\nData Encryption:\\nEncrypt data at rest and in transit to ensure security.\\nPrivate Deployment:\\nDeploy retrievers and generators on on-premises infrastructure or within secure VPCs for sensitive applications.\\nAuditing:\\nLog retrieval and generation activity for audit and compliance purposes.\\nEnterprise-Ready Implementation\\nSteps to Make RAG Enterprise-Ready:\\n\\nData Preparation:\\nCurate and preprocess enterprise knowledge bases for retrieval.\\nDomain Adaptation:\\nFine-tune both retriever and generator on domain-specific datasets.\\nInfrastructure:\\nLeverage scalable, secure, and cost-effective cloud or hybrid infrastructure.\\nMonitoring:\\nMonitor pipeline performance with observability tools like Prometheus or Grafana.\\nContinuous Improvement:\\nRegularly refine models, pipelines, and processes using user feedback and new datasets.\\nBy systematically addressing each challenge, RAG pipelines can deliver high-quality, reliable, and efficient results at the enterprise level.\",\n",
       "  'page_number': 1,\n",
       "  'file_name': 'dynamic_data.docx'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models for\\nKnowledge-Aware Response Planning\\nMinh Nguyen1⋆, Kishan K C2, Toan Nguyen2, Ankit Chadha2, and Thuy Vu2\\x00\\n1Department of Computer Science, University of Oregon, OR, USA\\n2Amazon Alexa AI, CA, USA\\nminhnv@cs.uoregon.edu\\n{ckshan,amztoan,ankitrc,thuyvu }@amazon.com\\nAbstract. Large Language Models (LLMs) have shown impressive emergent\\nlanguage capabilities, especially in applications with high ambiguity, such as lan-\\nguage reasoning and knowledge consolidation. However, previous work explores\\nthe use of LLMs for acquiring information using either parametric or external\\nknowledge, which might lead to serious issues such as hallucination. Toward\\nsolving these issues, we present a novel approach of knowledge-aware response\\nplanning (KARP) and propose a novel framework that employs (i) a knowledge re-\\ntriever to obtain relevant information from web documents or databases for a given\\nuser query, and (ii) a robust fine-tuning strategy for LLMs to exploit the retrieved\\nexternal knowledge for planning a final response. Experimental results show that\\nour proposed framework can provide natural, concise answers for open-domain\\nquestions with high accuracy.\\nKeywords: Knowledge-Aware Response Planning ·Question Answering ·Large\\nLanguage Models · Fine-tuning.\\n1 Introduction\\nGeneral question answering (QA), a crucial natural language processing (NLP) task, is\\noften regarded as AI-complete [62, 8]; that is, QA will only be considered solved once\\nall the challenging problems in artificial intelligence (AI) have been addressed. Several\\nvirtual response assistants, including Google Assistant, Amazon Alexa, and Apple’s\\nSiri, have integrated state-of-the-art QA technologies, allowing them to understand and\\ngenerate responses in natural languages, providing valuable services to users. However,\\ngeneral QA still presents significant challenges, primarily due to the inherent difficulties\\nin reasoning with natural language, including aspects like commonsense and general\\nknowledge. Past research has explored the use of Large Language Models (LLMs) for\\ngeneral QA, predominantly leveraging either parametric (e.g., ChatGPT3) or external\\n(e.g., WebGPT[38]) knowledge sources. This method, however, can lead to considerable\\ncomplications, including hallucination - the generation of plausible but incorrect or\\nunverified information. To address these challenges, this paper introduces the concept\\n⋆This work was completed while the author was an intern at Amazon Alexa AI.\\n3https://chat.openai.com/chat',\n",
       "  'page_number': 1,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '2 Nguyen et al.\\nq: What college offers chiropractic ?\\nc1:New York Chiropractic College offers 1 Chiropractic Degree program. It’s a private\\nuniversity in a far away town. In 2015, 173 students graduated in the study area of\\nChiropractic with students earning 173 Doctoral degrees.\\na1: New York Chiropractic college offers chiropractic.\\nc2:Chiropractic care is also essential for college students who want to stay healthy. The central\\nnervous system is based in the spinal column, so correcting subluxations (misalignments)\\nof the spine is important, no matter how old you are. Holt Chiropractic in Port Orchard,\\nWA provides expert chiropractic care to students of all ages.\\na2: Holt Chiropractic College offers chiropractic.\\nc3:Howell Township is a township in Monmouth County, New Jersey, United States. As\\nof the 2010 United States Census, the township’s population was 51,075, reflecting an\\nincrease of 2,172 from the 48,903 counted in the 2000 Census.\\na3: Howell Township College offers chiropractic.\\nTable 1: Generated answers for a question qwith different context passages c1(relevant),\\nc2(quasi-relevant), and c3(irrelevant) from MS MARCO QA NLG test set [39]. Answers\\na1,a2, and a3are generated by GenQA [17].\\nof Knowledge-Aware Response Planning (KARP) for general QA along with a novel\\nframework that combines a knowledge retriever with a robust fine-tuning strategy for\\nLLMs. In particular, the problem of KARP can be defined as follows. Given a user\\nquery and a prompt containing external knowledge, the goal is to develop a model that\\ncan consolidate a response that must be crafted not just from the externally sourced\\ninformation, but also from the model’s inherent parametric knowledge. This is different\\nfrom the previous work that aim to generate a response by either harnessing parametric\\nknowledge (e.g., ChatGPT) or retrieving from external knowledge such as knowledge\\nbases [2, 3, 65, 51], web documents [68, 6, 67, 7, 13], or a provided context [15, 45, 59,\\n10].\\nWith the emergent abilities of LLMs [61], generative QA systems, in which answers\\nare produced by a generative LLM, have been explored to improve the performance\\nof QA [21, 49, 43, 19, 27, 18, 12, 37]. In paritcular, previous work typically employs\\npre-trained LLMs with encoder-decoder architectures such as BART [28] and T5 [44],\\nwhere the encoder consumes a given question and a required relevant context as input for\\nthe decoder to generate an answer to the question [24, 17]. On one hand, the similarity\\nbetween generative QA and the pre-training tasks of LLMs enables transfer learning\\nto improve QA performance. On the other hand, the generative formulation allows for\\nflexibility in handling various types of QA problems (e.g., extractive QA, multiple-choice\\nQA) [24]. However, a well-known issue that has been shown to occur with the generative\\nmodels is hallucination [35, 50, 53], where the models generate statements that are\\nplausible looking but factually incorrect. Additionally, if the answers are composed by\\na pretrained LLM without external knowledge, i.e., using parametric knowledge, the\\ninformation contained in the answers might be outdated and no longer valid. For example,',\n",
       "  'page_number': 2,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 3\\nthe answer for the question “Which country is the reigning World Cup champion?” will\\nchange through time.\\nRecent works such as GenQA [17] and WebGPT [38] mitigate these issues by\\nemploying an information retrieval component, which is responsible for collecting web\\ncontent to compose an answer for a given question. Formally, given a question qand\\na retrieved web content c, the model is trained to take (q, c)as input to produce a\\nresponse a=fθ(q, c), where fθdenotes the corresponding LLM with the parameters θ.\\nUnfortunately, fθmay merely learn to copy/synthesize information from cto produce\\naifcoften contains necessary information for correctly answering the question qin\\ntraining data such as MS MARCO QA NLG [39]4. As a result, the model may fail\\nto provide a correct answer for a given question if the retrieved content is missing\\nor contains (quasi-)irrelevant information (see Table 1). In other words, performance\\nof these retrieval-based QA models are limited to an upper bound by the knowledge\\nretriever.\\nIn this work, we address such issues in building a generative QA model. First, we\\nutilize a knowledge retriever that employs Optimal Transport to selectively identify\\nrelevant content from web documents or databases for a given user query. Second,\\nwe propose a novel fine-tuning strategy specially designed for LLMs, which combines\\nexternal knowledge, i.e., provided by the knowledge retriever and the intrinsic pre-trained\\nknowledge in LLMs, wherever possible, to generate informed responses.\\nParticularly, we propose the knowledge retriever as a dense passage retriever (DPR)\\nmodel. Our proposed DPR model performs an alignment between a given question and\\na text passage via Optimal Transport to find relevant information in the passage for\\ndetermining its correctness. The relevant context in the passage will then be used to\\nproduce a correctness score for ranking. In this way, we can obtain top ktext passages\\nfrom databases/web documents, which are treated as external knowledge in our frame-\\nwork. Different from GenQA and WebGPT that follows a single-style “ a=fθ(q, c)”\\nfinetuning strategy, we propose to employ a multi-style finetuning strategy, where both\\n“a=fθ(q, c)” and “ a=fθ(q)” are used to train the model. The latter intentionally\\nexcludes the external knowledge cfrom the input to encourage the model to retrieve its\\nown knowledge from the model parameters θ, which have been pretrained on massive\\nunlabeled text data [28, 44, 11, 56]. To combine the two finetuning styles, we propose\\nto finetune the LLM with “ a=fθ(q, c)”, and sequentially finetune the model with\\n“a=fθ(q)”. At test time, we use the “ a=fθ(q, c)” style to make predictions. Ex-\\nperimental results show that our proposed finetuning strategy significantly improves\\nthe performance compared to the baselines on MS MARCO QA NLG, demonstrat-\\nting the effectiveness of our proposed method. Finally, we scale up our framework to\\nfurther improve the QA performance by training the model i) with “ a=fθ(q, c)” on\\nQA datasets such as SQUAD [45] ( cis a context passage), MCTest [48] ( cconsists of\\nmultiple choices), Anthropic [1] ( cis the previous question in a conversation), and ii)\\nwith “ a=fθ(q)” on QA datasets such as WikiQA [69] and Wdrass [73].\\n4All answers ain MS MARCO QA NLG are written by human annotators based on summarizing\\nanswer information in context passages c.',\n",
       "  'page_number': 3,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '4 Nguyen et al.\\nOur experiments show that the resulting system behaves as a knowledge aware\\nresponse planner that provides natural, concise answers for open-domain questions with\\nhigh accuracy.\\nFig. 1: Overview of our proposed framework for KARP. The blue and orange arrows\\nrepresent the finetuning and inference processes of our model respectively.\\n2 Proposed Method\\n2.1 Knowledge-Aware Response Planning\\nThe problem of Knowledge-Aware Response Planning (KARP) can be outlined as\\nfollows: Given a user query and a prompt loaded with external knowledge, the aim is to\\nbuild a model capable of formulating a response. This response should be planned not\\nonly from the external information provided but also derived from the model’s inherent\\nparametric knowledge.\\nTo this end, our proposed framework for KARP consists of (i) a knowledge retriever\\nand (ii) a generative LLM-based question answering model. An overview of our frame-\\nwork is shown in Figure 1. Details regarding the knowledge retriever and the generative\\nQA model are presented in section 2.2 and 2.3, respectively.\\n2.2 Knowledge Retriever\\nOur knowledge retriever functions as a dense passage retrieval (DPR) system. Given\\na question qand a group of Ntext passages C={c1, c2, . . . , c N}, the goal of DPR\\nis to determine the correct answer passages A⊂Cby learning a reranking function\\nr:Q×ϕ(C)→ϕ(C), where Qrepresents the set of questions and ϕ(C)represents all\\nthe possible orderings of C. The intent is to place the answer passages Aat the top of\\nthe ranking produced by the function r. The reranker ris typically a pointwise network\\nf(q, ci), such as TANDA [13], which learns to assign a correctness score pi∈(0,1)to\\neach text passage cifor ranking purposes. Our focus lies on the contextual DPR, where',\n",
       "  'page_number': 4,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 5\\nsupplementary context, like surrounding context, is used to more accurately ascertain\\nthe validity score of an answer passage.\\nOur knowledge retriever consists of three primary elements: i) Encoding, ii) Question-\\nContext Alignment with Optimal Transport (OT), and iii) Answer-Context Dependencies.\\nThe diagram of our suggested model can be seen in Figure 2.\\nEncoding We are provided with a question represented as q= [wq\\n1, wq\\n2, . . . , wq\\nTq]\\nwith Tqwords and a set of Ntext passages C={c1, c2, . . . , c N}retrieved from\\na search engine. Each passage, denoted as ci= [wc\\n1, wc\\n2, . . . , wc\\nTc], consists of Tc\\nwords. In this work, we consider previous and next passages cprev,cnext as additional\\ncontext for each candidate passage c∈C. To create the input for our DPR model,\\nwe concatenate the question, answer passage, and context passages into a single input\\nsequence: [q;c;cprev;cnext]. This combined sequence is then passed through a pre-\\ntrained language model (PLM), e.g., RoBERTa [33], to obtain contextualized word\\nembeddings. Additionally, we employ distinct segment embeddings for the question,\\nanswer passage, and context passages. These segment embeddings, which are randomly\\ninitialized and trainable during training, are added to the initial word embeddings in\\nthe first layer of the PLM. For simplicity, let [wq\\n1,wq\\n2, . . . , wq\\nTq]and[wc\\n1,wc\\n2, . . . , wc\\nTc]\\nrepresent the sequences of word representations obtained from the last layer of the PLM\\nfor the question qand the answer passage c∈C, respectively.\\nPretrained Language Model[CLS] question [SEP]   answer passage  [SEP] prev_context [SEP] next_contextInter-ContextDependencies\\nAlignmentvia Optimal TransportQuestionAnswer/Context\\n. . .\\nRelevant Context\\nGraph Convolutional Network\\nAS2 prediction\\nFig. 2: A diagram depicting the knowledge retriever in our framework for KARP.',\n",
       "  'page_number': 5,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '6 Nguyen et al.\\nQuestion-Context Alignment with OT In this section, we present our approach for\\nidentifying relevant context within the answer passage and its surrounding passages\\nbased on the alignment of words with the question. Specifically, we introduce the use\\nof Optimal Transport (OT) [36, 9] to address the task of aligning the question with the\\ncontext for DPR.\\nOT is a well-established technique used to transfer probability from one distribution\\nto another by establishing an alignment between two sets of points. In the discrete\\nsetting, we are provided with two probability distributions, denoted as pXandpY,\\ndefined over two sets of points, namely X={xi}n\\ni=1andY={yj}m\\nj=1(P\\nipxi= 1\\nandP\\njpyj= 1). Additionally, a distance function D(x, y) :X×Y→R+is given\\nto quantify the dissimilarity between any two points xandy. The objective of OT is to\\ndetermine a mapping that transfers the probability mass from the points in {xi}n\\ni=1to the\\npoints in {yj}m\\nj=1, while minimizing the overall cost associated with this transportation.\\nFormally, this involves finding the transportation matrix πXY∈R+n×mthat minimizes\\nthe following transportation cost:\\ndXY=X\\n1≤i≤n\\n1≤j≤mD(xi, yj)πXYij, (1)\\nso that πXY1m=pXandπT\\nXY1n=pY. The transportation matrix πXYsignifies the\\nbest matching between the sets of points XandY, where each row iin the matrix\\nindicates the optimal alignment from a point xi∈Xto each point yj∈Y.\\nIn our problem of aligning the question with the answer passage, we treat the question\\nqand the answer/context passage cas two point sets: {wq\\ni}Tq\\ni=1and{wc\\ni}Tc\\ni=1respectively\\n(each word is a point)5. To determine the probability distributions for these word sets, we\\npropose calculating the word frequencies and then normalizing the sum of frequencies.\\nSpecifically, the probability distribution for the question is obtained by:\\npwq\\ni=freq(wq\\ni)\\nPTq\\ni′=1freq(wq\\ni′)(2)\\nThe frequency freq(wq\\ni)corresponds to the number of occurrences of the word wq\\ni\\nin the training data’s questions. The same approach is applied to the answer/context\\npassage. To handle unseen words during testing, we utilize Laplace smoothing to assign\\na non-zero probability. Moving on, we estimate the distance between two words wq\\ni∈q\\nandwc\\nj∈cby measuring their semantic divergence, which involves computing the\\nEuclidean distance between their contextualized representations obtained from the PLM:\\nD(wq\\ni, wc\\nj) =||wq\\ni−wc\\nj||. The Sinkhorn-Knopp algorithm is then efficiently employed\\nto solve for the optimal transportation matrix πXY(in this case, πqcfor the question q\\nand the passage c) [55, 9]. Finally, we obtain the relevant context rcfor the passage cby\\ntaking the union of words wc\\njthat have the highest transportation probabilities:\\nrc=Tq[\\ni=1{wc\\nj|j=argmax1≤j′≤Tcπqcij′} (3)\\n5Before performing the alignment, we remove stopwords and punctuation marks from both sets\\nof words.',\n",
       "  'page_number': 6,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 7\\nTo compute the representation for the passage c, we take the average sum of the word\\nrepresentations within the relevant context:\\nrc=1\\n|rc|X\\nj|wc\\nj∈rcwc\\nj (4)\\nBy incorporating the relevant context, our intention is to eliminate any disruptive or\\nunrelated details from the passage representation.\\nAnswer-Context Dependencies For convenience, let [r1,r2,r3]denote the representa-\\ntions acquired from Equation (4) for the answer passage p1≡c, the previous passage\\np2≡cprev, and the next passage p3≡cnext. To capture the relationships between\\nthese passages, we view each passage as a node in a fully-connected graph G= (V, E),\\nwhere V={pi}(1≤i≤3)is the node set and E={(pi, pj)}(1≤i, j≤3) is the\\nedge set. Our objective is to determine a weight αij∈(0,1)for each edge (pi, pj)that\\nreflects the dependency of pionpj. To accomplish this, we propose to leverage their\\nsemantic representations ri,rj, and transportation costs to the question dqpi,dqpjto\\nmeasure the dependency weight αijbetween the passages piandpj. Specifically, we first\\ncompute the score: uij=FFN DEP([ri⊙rj;dqpi;dqpj]), where ⊙is the element-wise\\nproduct, [; ]represents the concatenation operation, and FFN DEP is a feed-forward\\nnetwork. Subsequently, the weight αijfor the edge (pi, pj)is obtained through a softmax\\nfunction:\\nαij=exp(uij)PK\\nj′=1exp(uij′)(5)\\nThe derived weights {αij}are subsequently utilized to enrich the passage representations\\nthrough Llayers of a Graph Convolutional Network (GCN) [25]:\\nhl\\ni=ReLU (KX\\nj=1αijWlhl−1\\nj+bl) (6)\\nwhere Wl,blare learnable weight matrix and bias for the layer lof the GCN ( 1≤l≤L),\\nandh0\\ni≡riis the input representation for the passage pi. The output vectors hL\\ni≡hiat\\nthe last layer of the GCN serve as the final representations for the passages pi. Intuitively,\\nthe weights αijenable each passage to decide the amount of information it receives\\nfrom the other passages to improve its representation for the task. The representation h1\\nfor the answer passage p1≡cis finally sent to a feed-forward network with a sigmoid\\noutput function to estimate the correctness score pc∈(0,1)for the answer passage\\nc:pc=FFN DPR(h1). For training, we minimize the binary cross-entropy loss with\\nthe correctness scores pc. At inference time, consistent with previous research [13], we\\ninclude all answer passages for each question for ranking.\\n2.3 Generative LLM-based Question Answering Model\\nBackground on Text Generation Finetuning Text generation finetuning has become a\\ngeneral approach to solving different NLP tasks, where input and expected output of a',\n",
       "  'page_number': 7,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '8 Nguyen et al.\\ntask can be represented as source and target text respectively for a generative model to\\nlearn the task [44, 34, 29]. For example, a pretrained generative LLM such as BART [28]\\nand T5 [44] can be finetuned on sentiment analysis by taking a text statement (e.g., “I\\nreally like the story” ) as source text to generate a text label (i.e., “Positive” ,”Negative” ,\\n“Neutral” ) to indicate the sentiment of the statement. As the text generation resembles the\\npretraining tasks (e.g., predicting next words) for the generative LLMs, the formulation\\ncould facilitate the transfer learning to the target task. In addition, it enables the data\\naugmentation method where training data for a task may also be leveraged for another\\ntask if the two tasks both are convertable to the text generation format [31]. These\\nadvantages have led to significant performance improvements for many NLP tasks such\\nas event extraction [31], named entity recognition [66], and dependency parsing [29].\\nSimilar to other NLP tasks, the generative methods have been explored for improving QA\\nperformance [21, 49, 43, 19, 27, 18, 12, 37]. To avoid hallucination and improve factual\\naccuracy for the models, recent works on generative QA employ the retrieval-based\\nmethods such as GenQA [17] and WebGPT [38].\\nGenQA is introduced by Hsu et al. [17] for generating appropriate answers for user\\nquestions instead of simply choosing the best answer candidate. This expands the answer\\nretrieval pipeline with an additional generation stage to produce correct and satisfactory\\nanswers, even in cases where a highly ranked candidate is not acceptable or does not\\nprovide a natural response to the question. In particular, GenQA employs a pretrained\\ngenerative LLM to produce an answer by taking a given question and a list of answer\\ncandidates as input, sorted by a trained reranker system.\\nWebGPT is designed by OpenAI researchers [38] to tackle the problem of long-form\\nquestion-answering, which involves generating a paragraph-length answer to an open-\\nended question. Specifically, WebGPT uses the Microsoft Bing Web Search API to\\nretrieve relevant documents for a given question. The model then interacts with a text-\\nbased environment where it can take actions such as clicking on links or opening new\\nweb pages to locate relevant passages from which to generate answers.\\nOur Proposed Finetuning Strategy The main goal of a general text-generation model\\nis to produce an output text sequence y= [y1, y2, . . . , y T]based on a given input text\\nsequence x= [x1, x2, . . . , x S], where the lengths of the input and output sequences\\nare denoted by SandT, respectively. With a pretrained encoder-decoder LLM such as\\nBART [28] or T5 [44], we can compute the conditional probability of P(y|x)for training\\nthe model. At test time, the decoder merges the previous output and input text to create\\nthe current output. A decoding algorithm such as Greedy or Beam Search [63] can be\\nused to generate an output text with the highest likelihood. For QA, given a question\\nqand a retrieved web content c(e.g., top answer passages), previous works such as\\nGenQA and WebGPT are trained to take (q, c)for as the source sequence to produce a\\nresponse as the target sequence a=fθ(q, c), where fθdenotes the corresponding LLM\\nwith the parameters θ. As a result, fθmay merely learn to copy/synthesize information\\nfrom cto produce aifcoften contains necessary information for correctly answering\\nthe question qin training data. Relying solely on the retrieved content c, the model',\n",
       "  'page_number': 8,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 9\\nmay fail to provide a correct answer for a given question if cis missing or contains\\nirrelevant/noisy information. In other words, performance of these retrieval-based QA\\nmodels are limited to an upper bound by the knowledge retriever.\\nDifferent from the previous works that follow a single-style “ a=fθ(q, c)” fine-\\ntuning strategy, we propose to employ a multi-style finetuning strategy, where both\\n“a=fθ(q, c)” and “ a=fθ(q)” are used to train the model. The latter intentionally\\nexcludes the external knowledge cfrom the input to encourage the model to retrieve its\\nown knowledge from the model parameters θ, which have been pretrained on massive\\nunlabeled text data [28, 44, 11, 56]. To combine the two finetuning styles, we propose\\nto finetune the LLM with “ a=fθ(q, c)”, and sequentially finetune the model with\\n“a=fθ(q)”. In this way, our model does not completely rely on the retrieval results to\\ngenerate answers for given questions. At test time, we use the “ a=fθ(q, c)” style to\\nmake predictions. The retrieved content cnow can be considered as a source of external\\nknowledge along with the pretrained knowledge contained in the model parameters θ\\nto generate an answer for the question. Under this perspective, we consider various QA\\ndatasets for each step in our finetuning process. We call such dataset collection OKQA\\nas they are publicly available and contains high-quality knowledge.\\nMS Marco QA NLG is a specialized version of the MS Marco dataset [39] that aims to\\nproduce natural language responses to user inquiries using web search result excerpts.\\nThis dataset includes 182Kqueries from Bing search logs, each is associated with top\\nten most relevant passages. A human annotator is then required to look at the passages\\nand synthesize an answer using the content of the passages that most accurately addresses\\nthe query.\\nSuper Natural Instructions (SNI) is a data collection proposed by [60]. The corpus\\nconsists of 1,616diverse NLP tasks and their expert-written instructions. In this work,\\nwe consider only question-answering tasks such as extractive QA with SQUAD [45]\\nand multiple-choice QA with MCTest [48]. For each task, we consider anything but a\\nquestion qprovided in the input as context c. Particularly, the context ccan be a passage,\\na fact, or a set of answer choices associated with the question. As a result, we obtain\\n180Kexamples for finetuning our model.\\nAnthropic is introduced by [1], containing conversations between a human and a com-\\nputer assistant. For each conversation, we consider a human question and the previous\\nquestion (if any) as the input sequence and the answer from the assistant as the output\\nsequence. As questions in a conversation are usually related to each other, the previous\\nquestion can be considered as a form of relevant context cfor clarifying the current\\nquestion q. Consequently, we obtain 280Kexamples for finetuning our model.\\nDense Passage Retrieval datasets, namely, WikiQA [69] and WDRASS [73] are also\\nused for finetuning our model. WikiQA is a collection of questions and answer candidates\\nthat have been manually annotated using Bing query logs on Wikipedia. WDRASS is\\na large-scale dataset of questions that are non-factoid in nature, such as questions that\\nbegin with “why” or “how”. The dataset contains around 64,000questions and over\\n800,000labeled passages that have been extracted from a total of 30Mdocuments. Each',\n",
       "  'page_number': 9,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '10 Nguyen et al.\\nquestion in such DPR datasets is associated with a set of answer candidates, in which\\nsome of the candidates are correct answers. As a question can have multiple correct\\nanswers, we select the longest answer as the output sequence for the question, which is\\nconsidered as the input sequence. This results in a set of 105Kexamples for finetuning\\nour model.\\nIn the end, the datasets where context is available for a question are employed in the\\nstep 1 of our finetuning process while the other datasets are used for further training the\\nmodel in the subsequent step. With a huge amount of various QA tasks, we expect this\\ncould teach the model to understand the nature of question answering and how to utilize\\nits own parametric knowledge (in case no context is provided) and external knowledge\\n(i.e., relevant context) to answer a given question.\\n3 Experiments\\n3.1 Benchmarking the Knowledge Retriever\\nExperimental Setup\\nDatasets We follow the previous work [13, 74] to conduct the evaluation. In particular,\\nwe use (i) WikiQA [69], consisting of questions from Bing query logs and manually\\nannotated answers from Wikipedia, and (ii) WDRASS [74], a large-scale web-based\\ndataset having factoid and non-factoid questions, to investigate our retrieval performance.\\nWe use the same train/dev/test splits used in previous work.\\nHyper-parameters and Tools In accordance with previous work, we use a small portion\\nof the WikiQA training data to tune hyper-parameters for our model and select the best\\nhyper-parameters for all the datasets [26]. We employ Adam optimizer to train the model\\nwith a learning rate of 1e-5and a batch size of 64. We set 400for the hidden vector\\nsizes for all the feed-forward networks, L= 2for the number of the GCN layers. We\\nuse Pytorch version 1.7.1 and Huggingface Transformers version 3.5.1 To implement\\nthe models. We use the NLTK library version 3.5 [4] to preprocess the data and remove\\nstopwords. The model performance is obtained over three runs with random seeds.\\nEvaluation Metrics We measure the model performance using the following standard\\nmetrics: Precision-at-1 (P@1) and Mean Average Precision (MAP) on the entire set of\\nanswer candidates for each question.\\nPerformance Comparison We compare our proposed model with TANDA [13], which\\nis the current state-of-the-art model. Table 2 shows the performance comparison between\\nthe models on two settings: i) using a non-finetuned RoBERTa-Base encoder, and ii) using\\na fine-tuned RoBERTa-Base encoder. The non-finetuned RoBERTa-Base is obtained\\nfrom [33] while the other is produced by fine-tuning TANDA on the ASNQ dataset\\n[13]. As can be seen from the table, all the models benefit from using the finetuned\\nRoBERTa-Base encoder. Across the two settings, our model outperforms the previous\\nmodels by large margins, demonstrating its effectiveness for the task.',\n",
       "  'page_number': 10,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 11\\nModelWikiQA WDRASS\\nw/o ASNQ with ASNQ with ASNQ\\nP@1 MAP P@1 MAP P@1 MAP\\nTANDA 63.24* 75.00* 78.67* 86.74* 54.60 63.50\\nOurs 74.16 83.29 83.77 89.28 55.9 61.8\\nTable 2: Performance comparison on WikiQA and WDRASS, * indicates results reported\\nby [26].\\nIn Table 2, we show the performance of our proposed model compared to TANDA\\non the WDRASS test set. As we can see, our knowledge retriever significantly improves\\nthe performance for P@1 score, however, decreases the performance for MAP score.\\nWe attribute this to the fact that questions in WDRASS dataset usually have more than\\n1 correct answers for a single question while our model ranks the answer candidates\\nindividually. However, we note that the top-1 answer candidate is often the most helpful\\nfor the answering process.\\n3.2 Evaluation for Knowledge-Aware Answer Generation\\nExperimental Setup\\nDataset We acquire the evaluation data as follows. First, we randomly select 2,000\\nquestions from the MS MARCO QA NLG test set. For each question, we rank all the\\ncontext passages using our model trained on WDRASS to obtain the top 5 candidates.\\nWe then concatenate the question and candidates to form the input, which is used to\\ngenerate the predicted answer.\\nEvaluation Metrics We employ widely-used evaluation metrics, including ROUGE [30],\\nBLEU [40], and BERTScore [72], for assessing the quality of generated answers in\\ncomparison to human-written natural answers. These metrics are commonly applied to\\nstandard text generation tasks such as summarization [71], machine translation [57], and\\nanswer generation [43].\\nIt is important to note that these metrics have their own limitations; however, these\\ncan be mitigated by providing more and higher-quality reference texts [5]. In the context\\nof answer generation, we enhance the reliability of these measurements by employ-\\ning human-written answers as references. Specifically, annotators create the reference\\nanswers used in this benchmark after being provided with the candidate responses.\\nPerformance Comparison Table 3 presents a comparison of three different config-\\nurations of KARP with GenQA model in terms of BLEU, RougeL, and BERTScore\\nmetrics.\\nThe results demonstrate that all three KARP configurations outperform the GenQA\\nmodel across all evaluation metrics. The best-performing configuration (config 2)\\nachieves a BLEU score of 39.4, a RougeL score of 0.608, and a BERTScore of 0.752.\\nThese results indicate that KARP offers a significant improvement over the GenQA',\n",
       "  'page_number': 11,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '12 Nguyen et al.\\nModel BLEU RougeL BERTScore\\nGenQA [17] 14.6 0.518 0.698\\nKARP (config 1) 38.3 0.632 0.762\\nKARP (config 2) 39.4 0.608 0.752\\nKARP (config 3) 38.9 0.604 0.750\\nTable 3: Comparison of our three KARP models trained with different hyper-parameter\\nsettings to GenQA [17].\\nmodel in the context of answer generation, which we attribute to our specialized fine-\\ntuning strategy for QA.\\n3.3 End-to-end Evaluation for Knowledge-Aware Response Planning\\nIn this section, we evaluate KARP in an end-to-end industry-scale scenario.\\nExperimental Setup We outline the experimental setup to evaluate the end-to-end\\nperformance of KARP in a web-scale scenario, involving tens of millions of web\\ndocuments. The configuration allows us to study the scalability and effectiveness of our\\napproach in a real-world, large-scale setting.\\nWeb Document Collection We constructed a large collection of web data, comprising\\ndocuments and passages, to facilitate the development of knowledge retrieval for end-\\nto-end system evaluation. This resource enables us to assess the impact of our work\\nin an industry-scale ODQA setting. We selected English web documents from the top\\n5,000 domains, including Wikipedia, from Common Crawl’s 2019 and 2020 releases.\\nThe pages were split into passages following the DPR procedure [23], limiting passage\\nlength to 200 tokens while maintaining sentence boundaries. This produced a collection\\nof roughly 100 million documents and 130 million passages. From this, we built (i) a\\nstandard Lucene/Elasticsearch index and (ii) a neural-based DPR index [23].\\nWeb-scale Knowledge Retrieval For each question, we retrieved up to 1,000 docu-\\nments/passages using both indexes. We then rank the passages and applied our knowl-\\nedge retriever to select relevant passages. We used top K= 5 candidates as external\\nknowledge for a question.\\nQuestion Sampling We randomly selected 2,000 questions from WDRASS test set as it\\nshows to represent natural questions extracted from the Web. In addition, the questions\\nwere also manually labeled.\\nBaselines We employ GenQA [17] as our main baseline in this experiment. We compare\\nthe performance of our system obtained by our proposed fine-tuning strategy and the\\nstandard fine-tuning (i.e., combining all datasets for finetuning) in a data parity setting.',\n",
       "  'page_number': 12,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 13\\nEvaluation Metrics We evaluate the performance of the end-to-end QA system using\\naccuracy metrics, i.e., the percentage of questions that were answered satisfactorily.\\nAdditionally, we define a correct answer as one that must not only be factually accurate,\\nbut also expressed in a natural and fluent manner. Answers that are too verbose or oddly\\nphrased are considered unsatisfactory.\\nPerformance Comparison The result show in the following table Table 4.\\nModel Accuracy\\nTANDA [13] baseline\\nGenQA [17] +2.20%\\nKARP →MS MARCO +6.20%\\nKARP →OKQA +7.40%\\nTable 4: Relative accuracy of different QA settings: TANDA [13], GenQA [17], and our\\nproposed frame work for KARP in two data configurations: MS MARCO (data parity)\\nand OKQA.\\nTable 4 presents the relative accuracy of different QA settings, including TANDA [13],\\nGenQA [17], and our proposed KARP with two data configurations: MS MARCO (data\\nparity) and (robust fine-tuning). From the table, we observe that GenQA outperforms\\nTANDA by 2.20%. Our proposed KARP model achieves even better results, with a\\n6.20% increase in accuracy when using the MS MARCO data configuration and a\\n7.40% increase in accuracy when using OKQAconfiguration. This demonstrates the\\neffectiveness of our proposed KARP model in various data settings.\\n4 Related Work\\nLarge Language Models (LLMs) LLMs have transformed NLP technologies with the\\nadvent of the Transformer architecture [57]. Two fundamental pre-training objectives,\\nMasked Language Modeling (MLM) and Causal Language Modeling (CLM), underpin\\nthe success of these models. MLM, introduced by BERT [10], predicts masked tokens in a\\nsentence using surrounding context, enabling LLMs to learn bidirectional representations\\nthat excel in various NLP tasks. In contrast, CLM, exemplified by GPT [41], predicts the\\nnext token in a sequence given its preceding context, showing remarkable success in text\\ngeneration and other downstream applications [42, 22, 43]. In this paper, we leverage the\\nCLM architecture for its language generation capabilities to enhance QA performance.\\nGeneral Question Answering using LLM A standard QA system consists of (i) a re-\\ntrieval engine that returns relevant knowledge and (ii) a model that generates a response\\naddressing the question, either through selection [52, 70, 13] or abstractive summariza-\\ntion of the top-selected answers [18, 12, 37]. In particular, recent summarization-based\\napproaches, e.g., GenQA [18, 12, 37], are highly susceptible to hallucination due to the',\n",
       "  'page_number': 13,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '14 Nguyen et al.\\nabsence of special treatment of irrelevant candidates, which commonly appear among\\nthe top-ranked options. As a result, the generated answer may seem plausible but could\\nbe factually incorrect [20, 76, 75, 64, 58, 54, 47, 46]. Even though its original goal is to\\ngenerate more natural answers, GenQA [18, 12, 37] can be considered as a method to\\nground LLMs for QA as it decodes an answer from the concatenation of both question\\nand answer candidates. This approach, however, requires good answer candidates and\\ncareful finetuning to reduce hallucinations.\\nWe propose, instead, a novel generation-based approach that leverages the emerging\\nlanguage reasoning capabilities of Large Language Models (LLMs) [41] to enhance\\nquality of generated answers. In particular, KARP is designed to mitigate the reliance\\non oracle data by making use of the context, such as all choices in multiple-choice QA,\\ninstead of a correct answer alone, i.e., the correct choice. The experiments demonstrated\\nthat our proposed framework for KARP is highly resilient to noisy input data, and bring\\nabout broader application across different QA tasks.\\nFine-tuning Strategies for LLMs Several fine-tuning strategies have been specifically\\nproposed for large language models (LLMs). These strategies can be broadly categorized\\ninto two groups: architecture-centric and data-centric. (i) Architecture-centric fine-tuning\\naims to improve the model’s robustness and adaptability by modifying hyper-parameters\\nacross layers. Gradual unfreezing [16] is one example, involving sequential fine-tuning\\nof model layers to prevent catastrophic forgetting and better adapt to downstream tasks.\\nLayer-wise learning rate decay [41] is another example, where different learning rates\\nare assigned to various layers to enable more refined adaptation to the target task.\\n(ii) Data-centric fine-tuning, on the other hand, concentrates on leveraging data from\\ndifferent sources or intermediate tasks to enhance model performance. Sequential fine-\\ntuning [14, 13] involves training the model on intermediate tasks before the final target\\ntask, improving its performance on the latter. Combining several related datasets for\\nmulti-task fine-tuning has also been shown to improve performance on the target task\\n[32]. Our work is related to data-centric fine-tuning. In particular, we propose a novel\\nstrategy specifically designed for the question answering context. By leveraging both\\nexternal knowledge and intrinsic parametric knowledge of LLMs, our approach aims to\\nenhance the quality of generated answers in QA tasks.\\n5 Conclusion\\nIn this paper, we presented a novel framework powered by large language models (LLMs)\\nfor KARP. To that end, we proposed an efficient fine-tuning strategy for KARP that\\nleverages (i) the emergent language reasoning abilities of LLMs and (ii) general question\\nanswering advances, including modelings and resources. Our experimental results show\\nthat KARP improves the state of the art in general QA tasks and outperforms vanilla\\nfine-tuning of LLMs in a dataset-parity setting. This research highlights the significance\\nof leveraging the intrinsic parametric knowledge of LLMs rather than relying solely on\\nconventional sequence-to-sequence fine-tuning, in order to improve their performance in\\nquestion answering tasks.',\n",
       "  'page_number': 14,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 15\\nReferences\\n1.Bai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S.,\\nGanguli, D., Henighan, T., et al.: Training a helpful and harmless assistant with reinforcement\\nlearning from human feedback. arXiv preprint arXiv:2204.05862 (2022)\\n2.Bao, J., Duan, N., Yan, Z., Zhou, M., Zhao, T.: Constraint-based question answering with\\nknowledge graph. In: Proceedings of COLING 2016, the 26th international conference on\\ncomputational linguistics: technical papers. pp. 2503–2514 (2016)\\n3.Bao, J., Duan, N., Zhou, M., Zhao, T.: Knowledge-based question answering as machine\\ntranslation. In: Proceedings of the 52nd Annual Meeting of the Association for Computational\\nLinguistics (V olume 1: Long Papers). pp. 967–976 (2014)\\n4.Bird, S., Klein, E., Loper, E.: Natural language processing with Python: analyzing text with\\nthe natural language toolkit. ” O’Reilly Media, Inc.” (2009)\\n5.Callison-Burch, C., Osborne, M., Koehn, P.: Re-evaluating the role of Bleu in machine\\ntranslation research. In: 11th Conference of the European Chapter of the Association for\\nComputational Linguistics. pp. 249–256. Association for Computational Linguistics, Trento,\\nItaly (Apr 2006)\\n6.Chen, D., Fisch, A., Weston, J., Bordes, A.: Reading wikipedia to answer open-domain\\nquestions. arXiv preprint arXiv:1704.00051 (2017)\\n7.Chen, D., Yih, W.t.: Open-domain question answering. In: Proceedings of the 58th annual\\nmeeting of the association for computational linguistics: tutorial abstracts. pp. 34–37 (2020)\\n8.Clark, P., Etzioni, O., Khot, T., Sabharwal, A., Tafjord, O., Turney, P., Khashabi,\\nD.: Combining retrieval, statistics, and inference to answer elementary science ques-\\ntions. Proceedings of the AAAI Conference on Artificial Intelligence 30(1) (Mar 2016).\\nhttps://doi.org/10.1609/aaai.v30i1.10325\\n9.Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transport. Advances in\\nneural information processing systems 26(2013)\\n10.Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. In: Proceedings of the 2019 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies, V olume 1 (Long and Short Papers). pp. 4171–4186. Association for Computational\\nLinguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1423\\n11.FitzGerald, J.G.M., Ananthakrishnan, S., Arkoudas, K., Bernardi, D., Bhagia, A., Bovi, C.D.,\\nCao, J., CHADA, R., Chauhan, A., Chen, L., Dwarakanath, A., Dwivedi, S., Gojayev, T.,\\nGopalakrishnan, K., Gueudre, T., Hakkani-T ¨ur, D., Hamza, W., Hueser, J., Jose, K.M., Khan,\\nH., Liu, B., Lu, J., Manzotti, A., Natarajan, P., Owczarzak, K., Oz, G., Palumbo, E., Peris,\\nC., Prakash, C.S., Rawls, S., Rosenbaum, A., Shenoy, A., Soltan, S., Harakere, M., Tan,\\nL., Triefenbach, F., Wei, P., Yu, H., Zheng, S., Tur, G., Natarajan, P.: Alexa teacher model:\\nPretraining and distilling multi-billion-parameter encoders for natural language understanding\\nsystems. In: KDD 2022 (2022)\\n12.Gabburo, M., Koncel-Kedziorski, R., Garg, S., Soldaini, L., Moschitti, A.: Knowledge transfer\\nfrom answer ranking to answer generation. In: Proceedings of the 2022 Conference on Empir-\\nical Methods in Natural Language Processing. pp. 9481–9495. Association for Computational\\nLinguistics, Abu Dhabi, United Arab Emirates (Dec 2022)\\n13.Garg, S., Vu, T., Moschitti, A.: Tanda: Transfer and adapt pre-trained transformer models for\\nanswer sentence selection. Proceedings of the AAAI Conference on Artificial Intelligence\\n34(05), 7780–7788 (Apr 2020). https://doi.org/10.1609/aaai.v34i05.6282\\n14.Gururangan, S., Marasovi ´c, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., Smith, N.A.:\\nDon’t stop pretraining: Adapt language models to domains and tasks. In: Proceedings of the',\n",
       "  'page_number': 15,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '16 Nguyen et al.\\n58th Annual Meeting of the Association for Computational Linguistics. pp. 8342–8360. Asso-\\nciation for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-\\nmain.740\\n15.Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom,\\nP.: Teaching machines to read and comprehend. Advances in neural information processing\\nsystems 28(2015)\\n16.Howard, J., Ruder, S.: Universal language model fine-tuning for text classification. In: Proceed-\\nings of the 56th Annual Meeting of the Association for Computational Linguistics (V olume 1:\\nLong Papers). pp. 328–339. Association for Computational Linguistics, Melbourne, Australia\\n(Jul 2018). https://doi.org/10.18653/v1/P18-1031\\n17.Hsu, C.C., Lind, E., Soldaini, L., Moschitti, A.: Answer generation for retrieval-based question\\nanswering systems. In: Findings of the Association for Computational Linguistics: ACL-\\nIJCNLP 2021. pp. 4276–4282. Association for Computational Linguistics, Online (Aug 2021).\\nhttps://doi.org/10.18653/v1/2021.findings-acl.374\\n18.Hsu, C.C., Lind, E., Soldaini, L., Moschitti, A.: Answer generation for retrieval-based question\\nanswering systems. In: ACL Findings 2021 (2021)\\n19.Izacard, G., Grave, E.: Leveraging passage retrieval with generative models for open domain\\nquestion answering. In: Proceedings of the 16th Conference of the European Chapter of\\nthe Association for Computational Linguistics: Main V olume. pp. 874–880. Association for\\nComputational Linguistics, Online (Apr 2021). https://doi.org/10.18653/v1/2021.eacl-main.74\\n20.Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y ., Ishii, E., Bang, Y .J., Madotto, A., Fung,\\nP.: Survey of hallucination in natural language generation. ACM Comput. Surv. 55(12) (mar\\n2023). https://doi.org/10.1145/3571730\\n21.Jiang, Z., Araki, J., Ding, H., Neubig, G.: Understanding and improving zero-shot multi-\\nhop reasoning in generative question answering. In: Proceedings of the 29th International\\nConference on Computational Linguistics. pp. 1765–1775. International Committee on Com-\\nputational Linguistics, Gyeongju, Republic of Korea (Oct 2022)\\n22.Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Rad-\\nford, A., Wu, J., Amodei, D.: Scaling laws for neural language models. arXiv preprint\\narXiv:2001.08361 (2020)\\n23.Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., Yih, W.t.: Dense\\npassage retrieval for open-domain question answering. In: Proceedings of the 2020 Conference\\non Empirical Methods in Natural Language Processing (EMNLP). pp. 6769–6781. Association\\nfor Computational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-\\nmain.550\\n24.Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., Hajishirzi, H.: UNI-\\nFIEDQA: Crossing format boundaries with a single QA system. In: Findings of the As-\\nsociation for Computational Linguistics: EMNLP 2020. pp. 1896–1907. Association for\\nComputational Linguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.findings-\\nemnlp.171\\n25.Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks.\\nIn: Proceedings of the 5th International Conference on Learning Representations (2017)\\n26.Lauriola, I., Moschitti, A.: Answer sentence selection using local and global context in\\ntransformer models. In: Advances in Information Retrieval: 43rd European Conference on\\nIR Research, ECIR 2021, Virtual Event, March 28–April 1, 2021, Proceedings, Part I. pp.\\n298–312. Springer (2021)\\n27.Lewis, M., Fan, A.: Generative question answering: Learning to answer the whole question.\\nIn: International Conference on Learning Representations (2019)\\n28.Lewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V .,\\nZettlemoyer, L.: BART: Denoising sequence-to-sequence pre-training for natural language',\n",
       "  'page_number': 16,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 17\\ngeneration, translation, and comprehension. In: Proceedings of the 58th Annual Meeting of the\\nAssociation for Computational Linguistics. pp. 7871–7880. Association for Computational\\nLinguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.703\\n29.Lin, B., Yao, Z., Shi, J., Cao, S., Tang, B., Li, S., Luo, Y ., Li, J., Hou, L.: Dependency parsing\\nvia sequence generation. In: Findings of the Association for Computational Linguistics:\\nEMNLP 2022. pp. 7339–7353. Association for Computational Linguistics, Abu Dhabi, United\\nArab Emirates (Dec 2022)\\n30.Lin, C.Y .: ROUGE: A package for automatic evaluation of summaries. In: Text Summarization\\nBranches Out. pp. 74–81. Association for Computational Linguistics, Barcelona, Spain (Jul\\n2004)\\n31.Liu, J., Chen, Y ., Liu, K., Bi, W., Liu, X.: Event extraction as machine reading comprehension.\\nIn: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP). pp. 1641–1651. Association for Computational Linguistics, Online (Nov 2020).\\nhttps://doi.org/10.18653/v1/2020.emnlp-main.128\\n32.Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natural language under-\\nstanding. In: Proceedings of the 57th Annual Meeting of the Association for Computational\\nLinguistics. pp. 4487–4496. Association for Computational Linguistics, Florence, Italy (Jul\\n2019). https://doi.org/10.18653/v1/P19-1441\\n33.Liu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer,\\nL., Stoyanov, V .: Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 (2019)\\n34.Lu, Y ., Lin, H., Xu, J., Han, X., Tang, J., Li, A., Sun, L., Liao, M., Chen, S.: Text2Event:\\nControllable sequence-to-structure generation for end-to-end event extraction. In: Proceed-\\nings of the 59th Annual Meeting of the Association for Computational Linguistics and\\nthe 11th International Joint Conference on Natural Language Processing (V olume 1: Long\\nPapers). pp. 2795–2806. Association for Computational Linguistics, Online (Aug 2021).\\nhttps://doi.org/10.18653/v1/2021.acl-long.217\\n35.Maynez, J., Narayan, S., Bohnet, B., McDonald, R.: On faithfulness and factuality in ab-\\nstractive summarization. In: Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics. pp. 1906–1919. Association for Computational Linguistics,\\nOnline (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.173\\n36.Monge, G.: M ´emoire sur la th ´eorie des d ´eblais et des remblais. Mem. Math. Phys. Acad.\\nRoyale Sci. pp. 666–704 (1781)\\n37.Muller, B., Soldaini, L., Koncel-Kedziorski, R., Lind, E., Moschitti, A.: Cross-lingual open-\\ndomain question answering with answer sentence generation. In: Proceedings of the 2nd\\nConference of the Asia-Pacific Chapter of the Association for Computational Linguistics and\\nthe 12th International Joint Conference on Natural Language Processing (V olume 1: Long\\nPapers). pp. 337–353. Association for Computational Linguistics, Online only (Nov 2022)\\n38.Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju,\\nV ., Saunders, W., et al.: Webgpt: Browser-assisted question-answering with human feedback.\\narXiv preprint arXiv:2112.09332 (2021)\\n39.Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng, L.: Ms marco:\\nA human generated machine reading comprehension dataset (November 2016)\\n40.Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation\\nof machine translation. In: Proceedings of the 40th Annual Meeting of the Association\\nfor Computational Linguistics. pp. 311–318. Association for Computational Linguistics,\\nPhiladelphia, Pennsylvania, USA (Jul 2002). https://doi.org/10.3115/1073083.1073135\\n41.Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language under-\\nstanding by generative pre-training (2018)\\n42.Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are\\nunsupervised multitask learners (2019)',\n",
       "  'page_number': 17,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '18 Nguyen et al.\\n43.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu,\\nP.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach.\\nLearn. Res. 21(1) (jan 2020)\\n44.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu,\\nP.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The\\nJournal of Machine Learning Research 21(1), 5485–5551 (2020)\\n45.Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: SQuAD: 100,000+ questions for machine\\ncomprehension of text. In: Proceedings of the 2016 Conference on Empirical Methods in\\nNatural Language Processing. pp. 2383–2392. Association for Computational Linguistics,\\nAustin, Texas (Nov 2016). https://doi.org/10.18653/v1/D16-1264\\n46.Raunak, V ., Menezes, A., Junczys-Dowmunt, M.: The curious case of hallucinations in\\nneural machine translation. In: Proceedings of the 2021 Conference of the North Amer-\\nican Chapter of the Association for Computational Linguistics: Human Language Tech-\\nnologies. pp. 1172–1183. Association for Computational Linguistics, Online (Jun 2021).\\nhttps://doi.org/10.18653/v1/2021.naacl-main.92\\n47.Rebuffel, C., Roberti, M., Soulier, L., Scoutheeten, G., Cancelliere, R., Gallinari, P.: Control-\\nling hallucinations at word level in data-to-text generation. CoRR abs/2102.02810 (2021)\\n48.Richardson, M., Burges, C.J., Renshaw, E.: MCTest: A challenge dataset for the open-domain\\nmachine comprehension of text. In: Proceedings of the 2013 Conference on Empirical Methods\\nin Natural Language Processing. pp. 193–203. Association for Computational Linguistics,\\nSeattle, Washington, USA (Oct 2013)\\n49.Roberts, A., Raffel, C., Shazeer, N.: How much knowledge can you pack into the parameters\\nof a language model? In: Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP). pp. 5418–5426. Association for Computational\\nLinguistics, Online (Nov 2020). https://doi.org/10.18653/v1/2020.emnlp-main.437\\n50.Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y ., Xu, J., Ott, M., Smith, E.M.,\\nBoureau, Y .L., Weston, J.: Recipes for building an open-domain chatbot. In: Proceedings of the\\n16th Conference of the European Chapter of the Association for Computational Linguistics:\\nMain V olume. pp. 300–325. Association for Computational Linguistics, Online (Apr 2021).\\nhttps://doi.org/10.18653/v1/2021.eacl-main.24\\n51.Saxena, A., Chakrabarti, S., Talukdar, P.: Question answering over temporal knowledge graphs.\\nIn: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\\nand the 11th International Joint Conference on Natural Language Processing (V olume 1:\\nLong Papers). pp. 6663–6676. Association for Computational Linguistics, Online (Aug 2021).\\nhttps://doi.org/10.18653/v1/2021.acl-long.520\\n52.Severyn, A., Moschitti, A.: Learning to rank short text pairs with convolutional deep neural\\nnetworks. In: Proceedings of the 38th international ACM SIGIR conference on research and\\ndevelopment in information retrieval. pp. 373–382 (2015)\\n53.Shuster, K., Poff, S., Chen, M., Kiela, D., Weston, J.: Retrieval augmentation reduces halluci-\\nnation in conversation. In: Findings of the Association for Computational Linguistics: EMNLP\\n2021. pp. 3784–3803. Association for Computational Linguistics, Punta Cana, Dominican\\nRepublic (Nov 2021). https://doi.org/10.18653/v1/2021.findings-emnlp.320\\n54.Shuster, K., Poff, S., Chen, M., Kiela, D., Weston, J.: Retrieval augmentation reduces halluci-\\nnation in conversation. In: Findings of the Association for Computational Linguistics: EMNLP\\n2021. pp. 3784–3803. Association for Computational Linguistics, Punta Cana, Dominican\\nRepublic (Nov 2021). https://doi.org/10.18653/v1/2021.findings-emnlp.320\\n55.Sinkhorn, R., Knopp, P.: Concerning nonnegative matrices and doubly stochastic matrices.\\nPacific Journal of Mathematics 21(2), 343–348 (1967)\\n56.Soltan, S., Ananthakrishnan, S., FitzGerald, J.G.M., Gupta, R., Hamza, W., Khan, H., Peris, C.,\\nRawls, S., Rosenbaum, A., Rumshisky, A., Prakash, C.S., Sridhar, M., Triefenbach, F., Verma,',\n",
       "  'page_number': 18,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Efficient Fine-tuning Large Language Models 19\\nA., Tur, G., Natarajan, P.: Alexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model. arXiv (2022)\\n57.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u.,\\nPolosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V ., Bengio, S., Wallach, H.,\\nFergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing\\nSystems. vol. 30. Curran Associates, Inc. (2017)\\n58. Wang, C., Sennrich, R.: On exposure bias, hallucination and domain shift in neural machine\\ntranslation. In: Proceedings of the 58th Annual Meeting of the Association for Computational\\nLinguistics. pp. 3544–3552. Association for Computational Linguistics, Online (Jul 2020).\\nhttps://doi.org/10.18653/v1/2020.acl-main.326\\n59.Wang, W., Yang, N., Wei, F., Chang, B., Zhou, M.: Gated self-matching networks for reading\\ncomprehension and question answering. In: Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (V olume 1: Long Papers). pp. 189–198 (2017)\\n60.Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y ., Mirzaei, A., Naik, A., Ashok, A.,\\nDhanasekaran, A.S., Arunkumar, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H., Purohit,\\nI., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Pal, K.K., Patel, M., Moradshahi, M.,\\nParmar, M., Purohit, M., Varshney, N., Kaza, P.R., Verma, P., Puri, R.S., Karia, R., Doshi, S.,\\nSampat, S.K., Mishra, S., Reddy A, S., Patro, S., Dixit, T., Shen, X.: Super-NaturalInstructions:\\nGeneralization via declarative instructions on 1600+ NLP tasks. In: Proceedings of the\\n2022 Conference on Empirical Methods in Natural Language Processing. pp. 5085–5109.\\nAssociation for Computational Linguistics, Abu Dhabi, United Arab Emirates (Dec 2022)\\n61.Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma,\\nM., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus,\\nW.: Emergent abilities of large language models. Transactions on Machine Learning Research\\n(2022), survey Certification\\n62.Weston, J., Bordes, A., Chopra, S., Rush, A.M., Van Merri ¨enboer, B., Joulin, A., Mikolov,\\nT.: Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint\\narXiv:1502.05698 (2015)\\n63.Wiseman, S., Rush, A.M.: Sequence-to-sequence learning as beam-search optimization. In:\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Process-\\ning. pp. 1296–1306. Association for Computational Linguistics, Austin, Texas (Nov 2016).\\nhttps://doi.org/10.18653/v1/D16-1137\\n64.Xiao, Y ., Wang, W.Y .: On hallucination and predictive uncertainty in conditional language\\ngeneration. In: Proceedings of the 16th Conference of the European Chapter of the Association\\nfor Computational Linguistics: Main V olume. pp. 2734–2744. Association for Computational\\nLinguistics, Online (Apr 2021). https://doi.org/10.18653/v1/2021.eacl-main.236\\n65.Xu, J., Wang, Y ., Tang, D., Duan, N., Yang, P., Zeng, Q., Zhou, M., Sun, X.: Asking clar-\\nification questions in knowledge-based question answering. In: Proceedings of the 2019\\nConference on Empirical Methods in Natural Language Processing and the 9th International\\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1618–1629 (2019)\\n66.Yan, H., Gui, T., Dai, J., Guo, Q., Zhang, Z., Qiu, X.: A unified generative framework for\\nvarious NER subtasks. In: Proceedings of the 59th Annual Meeting of the Association for\\nComputational Linguistics and the 11th International Joint Conference on Natural Language\\nProcessing (V olume 1: Long Papers). pp. 5808–5822. Association for Computational Linguis-\\ntics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.451\\n67.Yang, W., Xie, Y ., Lin, A., Li, X., Tan, L., Xiong, K., Li, M., Lin, J.: End-to-end open-\\ndomain question answering with BERTserini. In: Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics (Demonstrations).\\npp. 72–77. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019).\\nhttps://doi.org/10.18653/v1/N19-4013',\n",
       "  'page_number': 19,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': '20 Nguyen et al.\\n68.Yang, Y ., Yih, W.t., Meek, C.: Wikiqa: A challenge dataset for open-domain question an-\\nswering. In: Proceedings of the 2015 conference on empirical methods in natural language\\nprocessing. pp. 2013–2018 (2015)\\n69.Yang, Y ., Yih, W.t., Meek, C.: WikiQA: A challenge dataset for open-domain question\\nanswering. In: Proceedings of the 2015 Conference on Empirical Methods in Natural Language\\nProcessing. pp. 2013–2018. Association for Computational Linguistics, Lisbon, Portugal (Sep\\n2015)\\n70.Yoon, S., Dernoncourt, F., Kim, D.S., Bui, T., Jung, K.: A compare-aggregate model with latent\\nclustering for answer selection. In: Proceedings of the 28th ACM International Conference on\\nInformation and Knowledge Management. pp. 2093–2096 (2019)\\n71.Zhang, J., Zhao, Y ., Saleh, M., Liu, P.: PEGASUS: Pre-training with extracted gap-sentences\\nfor abstractive summarization. In: III, H.D., Singh, A. (eds.) Proceedings of the 37th In-\\nternational Conference on Machine Learning. Proceedings of Machine Learning Research,\\nvol. 119, pp. 11328–11339. PMLR (13–18 Jul 2020)\\n72.Zhang*, T., Kishore*, V ., Wu*, F., Weinberger, K.Q., Artzi, Y .: Bertscore: Evaluating text\\ngeneration with bert. In: International Conference on Learning Representations (2020)\\n73.Zhang, Z., Vu, T., Gandhi, S., Chadha, A., Moschitti, A.: Wdrass: A web-scale dataset\\nfor document retrieval and answer sentence selection. In: Proceedings of the 31st ACM\\nInternational Conference on Information & Knowledge Management. pp. 4707–4711 (2022)\\n74.Zhang, Z., Vu, T., Gandhi, S., Chadha, A., Moschitti, A.: Wdrass: A web-scale dataset\\nfor document retrieval and answer sentence selection. In: Proceedings of the 31st ACM\\nInternational Conference on Information and Knowledge Management. p. 4707–4711. CIKM\\n’22, Association for Computing Machinery (2022)\\n75.Zhao, Z., Cohen, S.B., Webber, B.: Reducing quantity hallucinations in abstractive sum-\\nmarization. In: Findings of the Association for Computational Linguistics: EMNLP\\n2020. pp. 2237–2249. Association for Computational Linguistics, Online (Nov 2020).\\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.203\\n76.Zhou, C., Neubig, G., Gu, J., Diab, M., Guzm ´an, F., Zettlemoyer, L., Ghazvininejad, M.:\\nDetecting hallucinated content in conditional neural sequence generation. In: Findings of the\\nAssociation for Computational Linguistics: ACL-IJCNLP 2021. pp. 1393–1404. Association\\nfor Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.findings-\\nacl.120',\n",
       "  'page_number': 20,\n",
       "  'file_name': 'Finetunung_LLM.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nPrinciples of Generative AI A Technical Introduction Generative artificial intelligence (GenAI) tools are an emerging class of new-age artificial intelligence algorithms capable of producing novel content — in varied formats such as text, audio, video, pictures, and code — based on user prompts. Recent advances in machine learning (ML), massive datasets, and substantial increases in computing power have propelled such tools to human-level performance on academic and professional benchmarks, 1comparable to the ninetieth percentile on the SAT and the bar exam. This rapid progress has led many to to believe that the metamorphosis of these technologies 2from research-grade demos to accessible and easy-to-use production-grade goods and services carries the potential to supercharge business processes and operations while enabling entirely new deliverables heretofore rendered infeasible by economic or technological factors. It took OpenAI’s ChatGPT, a conversational web app based on a generative (multimodal) language model, about five days to reach one million users (compared to 2.5 months for 3Instagram). On the business side, the Economist reports that the number of jobs mentioning AI-related skills quadrupled from 2022 to 2023. This enthusiasm has not gone unmet by investors. Generative AI startups reportedly raised 600% more capital in 2022 than in 2020.   4 \\n 1\\nFigure 1: A taxonomy of GenAI-related disciplines.',\n",
       "  'page_number': 1,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nPurpose and Scope  What are these new-era AI technologies? How do they function? What principles do they operate on? What makes them different than already-hyped-up conventional machine learning (ML) models? For what tasks is this class of technology most impactful? What future advances might one look forward to? These are the questions this report attempts to shed some light on. The report will also tease out how this understanding foundationally informs the best uses (and misuses) of GenAI in applied contexts. A word of disclaimer: this gradient of topics also means that, while the initial sections deal with factual, if somewhat simplified, nuts-and-bolt workings of such models, the later sections delve into hopefully reasonable, but in a manner that only time may attest to, extrapolations and speculations, as necessitated by the developing nature of this technology and its current phase in the technology adoption cycle. While generative AI models come in many different shapes, utilizing varied statistical and computational techniques to target various modalities, ranging from code and text to audio and video, this report focuses almost exclusively on large language models (LLMs) capable of generating novel text from textual prompts. This choice is partly due to the substantial lead LLMs have in driving the overall usage of generative AI models and partly due to the centrality 5of language in formulating and addressing commonplace information-processing tasks. That said, image- and code-based GenAI models have already witnessed successful commercial product deployment, for example, by Adobe for creating visual content and by Github as a programming assistance tool.   \\n 2\\nFigure 2: An image-based GenAI model, Midjourney’s response to the prompt — “Businessman in Tokyo amidst rush hour, his gaze fixed ahead, surrounded by a sea of black umbrellas.”\\nFigure 3: Based on a code-based GenAI model, OpenAI Codex, Github Copilot is a commercial tool that can generate functional code from specifications given as natural language. Reportedly, as of June 2023, it served over a million users.',\n",
       "  'page_number': 2,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nA Quick First Introduction to Language Models At its core, a language model implements a simple functionality— to predict the next word (or token) given a context window specifying preceding words. More precisely, given a context window, a language model outputs a probability distribution over all possible words in its vocabulary, indicating the probability with which each possible word follows the given list of words. Upon sampling a guess of the next word from the said distribution, the language model 6incrementally repeats this ostensibly primitive step to produce a more extensive body of text.     \\nWe make two observations here: 1.Completions are random. The predicted completion, given a context window, is not deterministic. Sampling the next word in each step from the output distribution introduces enough randomness to permit that the predicted completions could be meaningfully different on every fresh run. This stochasticity is why ChatGPT, for instance, can offer varied answers for the same prompt across successive runs. Replacing the sampling step with choosing (greedily) the most likely immediate word is known to degrade the quality of the produced text. The randomness in responses is also desirable from a user  3\\nFigure 4: A probabilistic model predicting the next word coupled with sampling can produce larger bodies of text.',\n",
       "  'page_number': 3,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nperspective in getting varied responses. From the deployer’s perspective, this optionally allows the model to gather user feedback regarding the quality of seemingly plausible responses. This choice partly also contributes to hallucination in language models. 2.Initial prompt matters. Language models are conditional probabilistic models. They produce a completion conditioned on the initial set of words. In this way, the initial context window, termed prompt, matters crucially to the produced completion. One hallmark of modern language models is that they keep track of the initial prompt even when generating large bodies of text, unlike the earlier generation of models, thus producing more coherent responses. Artful and cleverly crafted prompts can significantly improve the quality and utility of the synthesized text. Prompt engineering, for example, practices 7that encourage the language model to solve a problem by decomposing it into intermediate subproblems, has been known to improve the performance on logical reasoning tasks. Contextualizing LLMs in terms of Recent AI Advances Although we describe the text generation procedure above, many questions still need to be addressed: How do language models function internally? How are the output probabilities for the next word determined? What goes into creating (and indeed using) a language model? How are language models different from more traditional predictive models if all they do is predict the next token? We address these questions indirectly in the present section by taking a tour of the essential significant developments in machine learning and artificial intelligence that have occurred in the last decade and have fueled the creation of modern large language models. Classical Machine Learning as Prediction Machines We start with the most well-understood subset of machine learning techniques: supervised learning. The central objective in supervised learning is to produce a prediction rule that predicts well on unseen data, given enough labeled examples. For example, consider predicting house prices from the square footage in a given zip code. Instead of creating a hand-crafted prediction rule, the machine learning methodology advocates for choosing a prediction rule from an expressive but non-exhaustive class of rules, such as linear predictors, that provides the best fit on an existing collection of size-price examples. The statistically well-substantiated leap of faith here is that we expect (or at least hope) that a parsimonious prediction rule that predicts well on collected data, for which we know the correct answers, continues to maintain its predictive edge on unseen data, where answers or prices are unknown. Such a predictive methodology benefits from an abundance of labeled examples, hoping that a prediction rule learned from more examples is more robust in that its superior predictive performance on seen data is less ascribable to chance alone. Another example of a supervised learning task is to separate spam from non-spam mail, given the text in email messages. Again, having more examples of spam and non-spam emails is helpful to a supervised learning algorithm.  4',\n",
       "  'page_number': 4,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\n Characteristics common to both language models and supervised learning: 1.Predicting Well is the Yardstick. A prediction rule is good as long as it makes reasonable predictions on average. Compared to more ambitious sub-disciplines in statistics, any statements about causality, p-values, and recovering latent structure are absent. We are similarly impervious to such considerations in language models. Such simplicity of goals enables very flexible prediction rules in machine learning. Although seeming modest in its aim, the art of machine learning has long been to cast as many disparate problems as questions about prediction as possible. Predicting house prices from square footage is a regular regression task. But, for reverse image captioning, is “predicting” a (high-dimensional) image given a few words a reasonable or well-defined classification task? Yet, this is how machine learning algorithms function. 2.Model Agnosticism. Supervised learning algorithms realize the adage that all models are wrong, but some are useful. For example, when building the price predictor above, a data scientist does not believe that the genuine relationship between prices and area is linear or well-specified. Similarly, when using neural networks to predict the next word in language models, we don’t believe that this is how Shakespeare must have employed a neural network to compose his texts. Yet, there are crucial differences:  5\\nFigure 5: Predicting house prices from square footage. Pictured is a linear regression, an example of a supervised learning algorithm that uses extant data to learn a linear predictor.',\n",
       "  'page_number': 5,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\n1.Fidelity of Seen Data vs. Unseen Data. Classical supervised learning operates on the assumption that seen data must be representative of unseen data in a particular sense, namely that any fixed example is equally likely to be in the seen or unseen bucket. In the absence of temporal effects, this is reasonable for house prices. More generally, supervised learning requires a well-curated dataset that is closely aligned with the prediction task at hand. But, as we will see, language models are trained on vast corpora of somewhat ruthlessly collected texts from the internet. Yet, completing a random partial sentence from the internet is presumably not what businesses using language models care about. Deep Learning as Automated Representation Learning Although useful for panel or tabular data, pre-deep-learning-era supervised algorithms struggled to predict well when presented with visual or auditory inputs. Although the promise of machine learning is predicated on the automation of learning, in practice, supervised learning algorithms require carefully crafted representations of input data in which operations like additions and multiplications, for example, for linear regression, were semantically relevant. Decades of painstaking research in signal processing and computer vision had resulted in domain-specific hand-crafted representations, each useful for a specific modality (images, audio, or video). The predictive performance of ML algorithms was limited by how good such representations were. \\n 6\\nFigure 6: A typical deep neural network for recognizing faces. Each successive layer progressively learns higher-level representations (from edges to contours to faces).',\n",
       "  'page_number': 6,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nThe revolution in deep learning was to automate the process of representation learning itself. Deep learning uses neural networks with multiple layers, each layer incrementally converting the data into a more manageable form, all to make better predictions. This form of automated hierarchical representation learning heralded a decade of tremendous progress in image and speech recognition and machine translation, starting with the breakthrough work of Krizhevsky, Sutskever, and Hinton in 2012 on the Imagenet challenge. Taking advantage of GPUs (a form 8of shared-memory parallel computing) and the availability of a large public dataset, this seminal work slashed the error rate for image recognition by a substantial multiple. Parallel gains were later realized using similar deep neural network architectures in speech recognition and other machine learning domains. In this sense, the advances deep learning enabled were (relatively) domain agnostic. Although deep neural networks are data-hungry in that they require a substantially large dataset to start predicting well, they also successfully realize a long-promised advantage of neural networks. This factor is crucial to the practice of modern-day machine learning. In the process of hierarchically learning representations, deep nets learn task- (or label--) agnostic features of the dataset in the lower layers, while higher layers closer to the output account for task-specific representations. This permits us to (a) train a deep net to separate images of cats and dogs on a large dataset and (b) subsequently build a shallow (even linear) performant neural net that uses the lower layers of the former to craft useful representations to classify images of zebra and giraffes. Step A is often called pre-training, and step B is referred to as supervised fine-tuning. This manner of amortizing the learning across tasks that are not individually data-rich is central to language models. Word Embeddings and Contrastive Learning While the progress of deep learning in speech and audio was made possible by the availability of large crowd-labeled datasets (with 10s of millions of annotated images), such large high-quality datasets were absent in the textual domain, despite a plethora of unlabelled data in the form of books, Wikipedia articles, and articles on the internet. Could a machine learning algorithm make use of the cheap, unlabelled data instead? In computational linguistics, the distributional hypothesis codifies an appealing and intuitive idea that similar words occur in similar contexts. In 2013, inspired by this observation, Mikolov et al 9trained a neural network, termed Word2Vec, to predict randomly selected words in a text corpus given neighboring words for each. Note that this step doesn’t require any need human annotators. They observed that the 300-dimensional vector representations the neural net learned for words had excellent linear algebraic properties that transparently reflected the underlying semantics. For example, one obtained Queen when queried for the word with the vector closest to King - Man + Woman. Thus, each vector dimension captured some abstract semantic degree of freedom. These representations were also valuable for natural classification tasks with limited data, such as sentiment classification, given a small number of examples.  7',\n",
       "  'page_number': 7,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\n The approach of creating auxiliary labeling tasks for free from unlabelled data to learn semantically relevant representation is called contrastive learning and has proved helpful in other domains, too. For example, given a set of unlabelled images, a classifier trained to recognize random crops from the same image as a positive match and those from distinct images as a negative match (pre-training step) learns representations useful for supervised fine-tuning on genuine classification tasks downstream. Transformers mollify the Optimization Landscape While word embeddings serve as proof that textual semantic regularities can be assessed without labeled data, substantive language processing tasks need an algorithmic implementation of the concept of memory to capture relationships between words that are positionally far apart. For example, a common motif in stories is that the next act derives from some event that occurred a while ago.  \\n 8\\nFigure 7: Vector space representations of words exhibit linear algebraic relationships between semantic units and can be used to answer analogy questions, e.g., son - father + mother = daughter.\\nFigure 8: RNNs capture memory effects by sequentially processing information.',\n",
       "  'page_number': 8,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nThe first generation of neural networks that captured the notion of memory were Recurrent Neural Networks (RNNs), by sequentially processing a piece of text one word at a time while updating an internal state to maintain continuity, a proxy for memory. Unfortunately, optimizing such recurrent neural nets to find one that best fits a given dataset proved extra-ordinarily error-prone and challenging. In 2017, Vaswani et al introduced a different neural network architecture, termed transformer, 10that could efficiently capture long-range relations between tokens compactly (non-sequentially) by processing the entire surrounding context window at once while remaining amenable to gradient-based optimization. The introduction of transformers spurred a line of research on language models, culminating in training models with an increasingly higher number of parameters trained on ever larger datasets. For example, GPT2 (Generative Pre-trained Transformer 2), released in 2019, is a 1.5 billion parameter model trained on 40 GB of data, while GPT3, released in 2020, is a 175 billion parameter model trained on 570 GB of text data. While larger models resulted in better performance, the open-market cost for training these enormous models was estimated to be tens of millions of dollars.  \\nGeneral-Purpose Language Models: Supervised Fine-tuning & GPT3 The general paradigm brought about by contrastive learning was first to learn a large model on auxiliary tasks created using an unlabelled dataset (the pre-training step) and subsequently to use these learned representations in a downstream supervised learning task given a few task-specific labeled examples (the supervised fine-tuning step). While broadly useful and practical, supervised fine-tuning requires replicas of the baseline pre-trained model for each downstream  9\\nFigure 9: The LLM arms race with exponentially increasing parameter counts. (Credit: HuggingFace)',\n",
       "  'page_number': 9,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\ntask; further, the large size of language models makes running even a few steps of gradient-based iterative optimization for supervised learning prohibitive except on computationally expensive hardware setups. The paper describing the architecture of the GPT3 model presents a far cheaper and more 11convenient way of repurposing pre-trained language models for specific downstream tasks, namely, by specifying a few labeled examples in the prompt before asking for a label or response for unseen data. This mode of inference, in-context learning, does not require computationally expensive adjustments to the weights or parameters of an LLM and instead treats the entire downstream supervised task as a prompt for the language model to complete. This makes LLMs very attractive for end-users, who no longer have to create copies of the large model to customize, nor do they have to run a sophisticated optimization procedure to adjust parameters; each downstream task, in effect, becomes a conversation. While fine-tuning may still result in additional performance gains over in-context learning for some tasks in exchange for a massive increase in computational load, a crucial advance of GPT3 is that this substantially lowers this gap, democratizing the use (although not the training) of LLMs.  \\n 10\\nFigure 10: An illustration of in-context learning. GPT4 figures out the correct pattern that the answer is the first number + reverse of the second, given two examples.',\n",
       "  'page_number': 10,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\nTowards Conversational AI: Learning from Human Feedback While GPT3-like models happen to be good at conversation-centered tasks, they are not explicitly trained or incentivized to follow instructions. OpenAI’s InstructGPT model post pre-12training aligns the model to follow the users’ instructions by fine-tuning the model to mimic labeled demonstrations of the desired behavior (via supervised learning) and highly-ranked responses to prompts as collected using human feedback (via reinforcement learning).  \\nThe Future: Foundation Models Given the success of language models, there has been increased interest in the possibility of recreating the magic of LLMs in other domains. Such models, generically termed foundation models, attempt to amortize the cost of limited-data downstream tasks by pre-training on large corpora of broadly related tasks or unlabelled datasets. For example, one might be able to repurpose the LLM paradigm to train a generalist robot or decision-making agent that learns from supply chain operations across all industries. Conclusion This report contextualizes large-language models within the more extensive machine learning and artificial intelligence landscape by training the origins of the principal ideas that fuel today’s large language models. By bringing out their essential characteristics and differences against traditional modes of machine learning, we hope that a user of such models can be better  11\\nFigure 11: While GPT3 performs text completion by guessing the most plausible completion, InstructGPT has been explicitly trained to follow instructions. (Credit: OpenAI’s web report)',\n",
       "  'page_number': 11,\n",
       "  'file_name': 'genai-principles.pdf'},\n",
       " {'chunk': 'Karan Singh, Assistant Professor of Operations Research \\ninformed of the underlying tradeoffs such models induce, e.g., the performance-resource tradeoffs between fine-tuning and in-context learning. Endnotes  See the ﬁrst table on OpenAI’s announcement for an overview of GPT4’s performance on other academic, 1professional and programming exams. The quoted nineMeth percenMle performance on the bar exam was assessed by Katz et al, but others have raised concerns.  See quotes by industry and research leaders here.2 See iniMal consumer adopMon staMsMcs for ChatGPT here and here.3 See this reporMng for investments in GenAI.4 See current and project user bases for GenAI here.5 When producing text, rather than sampling the next word incrementally, a more systemaMc search operaMon 6termed Beam Search, coined by Raj Reddy at CMU, oXen yields beYer results. Structuring iniMal text to elicit useful outputs from GenAI model is called prompt engineering.7 See the full Krizhevshy, Sutskever, Hinton paper here.8 See the Word2Vec paper here.9 See the paper that introduced Transformers here.10 See the GPT3 paper here.11 See the instruct GPT paper here.12\\n 12',\n",
       "  'page_number': 12,\n",
       "  'file_name': 'genai-principles.pdf'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e90643d-76d0-4515-bab7-950e3c5d7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "MODEL_NAME= \"all-mpnet-base-v2\"\n",
    "def generate_embeddings(chunks, model_name=MODEL_NAME):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2e5d6ed3-8537-4a70-acff-f2f596f870d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b56593117b444fc9058f8ab1c216b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings=generate_embeddings(chunks_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c931f373-b2fc-4a9a-80c5-f0bd1e5e0acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06741509,  0.03627042, -0.00124236, ...,  0.0187794 ,\n",
       "        -0.04226869, -0.01900765],\n",
       "       [ 0.08236661,  0.02293764, -0.02640497, ...,  0.03725887,\n",
       "        -0.12297844, -0.01474249],\n",
       "       [ 0.0853631 ,  0.01932502, -0.02593875, ...,  0.04847268,\n",
       "        -0.08994223, -0.06346259],\n",
       "       ...,\n",
       "       [ 0.05958821,  0.02113394, -0.00898292, ...,  0.03281295,\n",
       "        -0.06478857, -0.0457931 ],\n",
       "       [ 0.06739228,  0.05418719, -0.02677693, ...,  0.03942776,\n",
       "        -0.0771951 , -0.0341876 ],\n",
       "       [ 0.01247031,  0.01009415, -0.04380452, ..., -0.02829326,\n",
       "        -0.05914651, -0.0624537 ]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "25931ff6-f338-48f7-836c-341e8119b6d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection RAG_embeddings already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mClient(Settings(persist_directory\u001b[38;5;241m=\u001b[39mCHROMA_DB_DIR))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COLLECTION_NAME \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mlist_collections():\n\u001b[1;32m----> 9\u001b[0m     collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_collection(name\u001b[38;5;241m=\u001b[39mCOLLECTION_NAME)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\client.py:147\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[1;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    145\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 147\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[0;32m    156\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[0;32m    157\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    158\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    159\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[0;32m    160\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\segment.py:103\u001b[0m, in \u001b[0;36mrate_limit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rate_limit_enforcer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrate_limit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\rate_limit\\simple_rate_limit\\__init__.py:23\u001b[0m, in \u001b[0;36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\api\\segment.py:226\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    215\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43msegments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Passing empty till backend changes are deployed.\u001b[39;49;00m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is lazily populated on the first add\u001b[39;49;00m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n\u001b[0;32m    239\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mprepare_segments_for_new_collection(coll)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\chromadb\\db\\mixins\\sysdb.py:241\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[1;34m(self, id, name, configuration, segments, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[0;32m    236\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection\u001b[38;5;241m.\u001b[39mid, tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[0;32m    237\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m    245\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    252\u001b[0m )\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n",
      "\u001b[1;31mUniqueConstraintError\u001b[0m: Collection RAG_embeddings already exists"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "CHROMA_DB_DIR = \"chroma_db\"  # Directory to store the Chroma database\n",
    "COLLECTION_NAME=\"RAG_embeddings\"\n",
    "\n",
    "client = chromadb.Client(Settings(persist_directory=CHROMA_DB_DIR))\n",
    "if COLLECTION_NAME not in client.list_collections():\n",
    "    collection = client.create_collection(name=COLLECTION_NAME)\n",
    "else:\n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Prepare metadata and upload to ChromaDB\n",
    "for i, (chunk_metadata, embedding) in enumerate(zip(chunks_with_metadata, embeddings)):\n",
    "    metadata = {\n",
    "        \"filename\": chunk_metadata[\"file_name\"],\n",
    "        \"chunk_index\": i,\n",
    "        \"page_number\": chunk_metadata[\"page_number\"],\n",
    "        \"text\": chunk_metadata[\"chunk\"]\n",
    "    }\n",
    "    pdf_file=chunk_metadata[\"file_name\"]\n",
    "    collection.add(\n",
    "        \n",
    "        ids=[f\"{pdf_file}_chunk_{i}\"],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[metadata]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463ae9c-5698-4541-bfb3-5eb3a609ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06b218-0b25-4fab-8fb6-a06d52208851",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"RAG_embeddings\"  # Replace with your collection name\n",
    "\n",
    "if COLLECTION_NAME in [col.name for col in client.list_collections()]:\n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    \n",
    "    # Fetch and print details about the collection\n",
    "    print(f\"Collection: {COLLECTION_NAME}\")\n",
    "    \n",
    "    # Fetch metadata for all items\n",
    "    items = collection.get(include=[\"metadatas\"])\n",
    "    print(\"Metadata and Documents in the collection:\")\n",
    "    for metadata in items[\"metadatas\"]:\n",
    "        print(f\"Metadata: {metadata}\")\n",
    "       \n",
    "else:\n",
    "    print(f\"Collection {COLLECTION_NAME} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fad9a-ab15-4439-bb3f-607ed46fa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(directory):\n",
    "    \"\"\"Return a set of files in the directory.\"\"\"\n",
    "    return {file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))}\n",
    "\n",
    "def detect_changes(directory, known_files):\n",
    "    \"\"\"Detect new and deleted files in the directory.\"\"\"\n",
    "    current_files = get_existing_files(directory)\n",
    "    new_files = current_files - known_files\n",
    "    deleted_files = known_files - current_files\n",
    "    return new_files, deleted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5bfd4b-2b49-4fca-8d83-5a06f210c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_files = get_existing_files(PDF_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547e525-69c9-4a5d-96df-1a24b33c5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c20745-96da-4e78-873f-36b462c0747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_files, deleted_files = detect_changes(PDF_FOLDER, known_files)\n",
    "print(new_files)\n",
    "print(deleted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6f92d-bf58-436f-8450-6e3393c988e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if new_files:\n",
    "    text_by_page=[]\n",
    "    print(f\"New files detected: {new_files}\")\n",
    "    for file_name in new_files:\n",
    "        file_processing(file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addce8a-536b-4f44-adf8-523f09983d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_by_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a28dc-3503-4597-b5d5-2e034720b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_chunks_with_metadata=chunk_text_with_pages(text_by_page, chunk_size=CHUNK_SIZE, overlap_size=OVERLAP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0b8e7-e7e0-4693-bd3f-f7e039ae76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14b8bb-633b-4ec3-ad66-3572a750f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings=generate_embeddings(updated_chunks_with_metadata)\n",
    "new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659b32c-0b2d-4253-ba22-e890aa8a60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (chunk_metadata, embedding) in enumerate(zip(updated_chunks_with_metadata, new_embeddings)):\n",
    "    metadata = {\n",
    "        \"filename\": chunk_metadata[\"file_name\"],\n",
    "        \"chunk_index\": i,\n",
    "        \"page_number\": chunk_metadata[\"page_number\"],\n",
    "        \"text\": chunk_metadata[\"chunk\"]\n",
    "    }\n",
    "    pdf_file=chunk_metadata[\"file_name\"]\n",
    "    collection.add(\n",
    "        \n",
    "        ids=[f\"{pdf_file}_chunk_{i}\"],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[metadata]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c960f49-ca71-4122-a5ed-107deb0e786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"RAG_embeddings\"  # Replace with your collection name\n",
    "\n",
    "if COLLECTION_NAME in [col.name for col in client.list_collections()]:\n",
    "    collection = client.get_collection(name=COLLECTION_NAME)\n",
    "    \n",
    "    # Fetch and print details about the collection\n",
    "    print(f\"Collection: {COLLECTION_NAME}\")\n",
    "    \n",
    "    # Fetch metadata for all items\n",
    "    items = collection.get(include=[\"metadatas\"])\n",
    "    print(\"Metadata and Documents in the collection:\")\n",
    "    for metadata in items[\"metadatas\"]:\n",
    "        print(f\"Metadata: {metadata['filename']}\")\n",
    "       \n",
    "else:\n",
    "    print(f\"Collection {COLLECTION_NAME} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061049e-eba6-4bd5-8e28-74dca63c4c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
